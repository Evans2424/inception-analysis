{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-eda-title",
   "metadata": {},
   "source": [
    "# INCEpTION Annotations Exploratory Data Analysis\n",
    "## Comprehensive Analysis of Portuguese Municipal Documents\n",
    "\n",
    "**Objective**: Provide statistical foundations and publication-ready visualizations for academic research on Named Entity Recognition (NER) systems applied to Portuguese municipal governance documents.\n",
    "\n",
    "**Dataset**: 120+ manually annotated documents from multiple Portuguese municipalities using the INCEpTION annotation platform.\n",
    "\n",
    "**Analysis Focus**: \n",
    "- Entity type distributions and characteristics\n",
    "- Posicionamento (voting positioning) patterns\n",
    "- Assunto analysis\n",
    "- Cross-municipality comparisons\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cp1kkedwsci",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter configuration for optimal display\n",
    "%matplotlib inline\n",
    "\n",
    "# Verify environment\n",
    "import sys\n",
    "print(f\"Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Running in: {'Jupyter' if 'ipykernel' in sys.modules else 'Standard Python'}\")\n",
    "print(\"\u2705 Jupyter magic commands loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "\n",
    "# Data analysis and statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, kruskal, mannwhitneyu\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom utilities\n",
    "sys.path.append('./utils')\n",
    "from inception_parser import InceptionParser\n",
    "from analysis_functions import AnnotationAnalyzer, calculate_effect_size, bootstrap_confidence_interval\n",
    "from figure_utils import save_figure, save_html_only, setup_colab_kaleido\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "px.defaults.template = \"plotly_white\"\n",
    "px.defaults.color_continuous_scale = \"viridis\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully\")\n",
    "print(f\"\ud83d\udcca Analysis environment ready\")\n",
    "print(f\"\ud83d\udcc5 Analysis date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\ud83d\uddbc\ufe0f Matplotlib backend: {matplotlib.get_backend()}\")\n",
    "print(\"\ud83d\udcbe Colab-friendly figure utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "INCEPTION_DATA_DIR = Path('./inception')\n",
    "RESULTS_DIR = Path('./results')\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "STATISTICS_DIR = RESULTS_DIR / 'statistics'\n",
    "\n",
    "# Create directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STATISTICS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\ud83d\udcc2 Data directory: {INCEPTION_DATA_DIR}\")\n",
    "print(f\"\ud83d\udcca Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "# Check data availability\n",
    "json_files = list(INCEPTION_DATA_DIR.glob('*.json'))\n",
    "print(f\"\ud83d\udcc4 Found {len(json_files)} INCEpTION JSON files\")\n",
    "\n",
    "if len(json_files) == 0:\n",
    "    print(\"\u26a0\ufe0f No JSON files found. Please check the data directory path.\")\n",
    "else:\n",
    "    print(\"\u2705 Data files detected\")\n",
    "    print(f\"\ud83d\udccb Sample files: {[f.name for f in json_files[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394da04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure figure saving for Colab compatibility\n",
    "print(\"\ud83d\udd27 Configuring figure saving...\")\n",
    "SAVE_HTML = True  # Always save HTML (works in all environments)\n",
    "SAVE_IMAGES = False  # Disable PNG exports by default (can cause issues in Colab)\n",
    "\n",
    "# Uncomment the next line if you want to try setting up Kaleido for image exports\n",
    "# setup_colab_kaleido()\n",
    "\n",
    "print(f\"\ud83d\udcbe HTML export: {'\u2705 Enabled' if SAVE_HTML else '\u274c Disabled'}\")\n",
    "print(f\"\ud83d\uddbc\ufe0f PNG export: {'\u2705 Enabled' if SAVE_IMAGES else '\u274c Disabled (Colab-friendly)'}\")\n",
    "print(\"\ud83d\udca1 Change SAVE_IMAGES to True if you need PNG exports and have Kaleido working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parsing-section",
   "metadata": {},
   "source": [
    "## 2. Data Parsing and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the INCEpTION parser\n",
    "print(\"\ud83d\udd04 Initializing INCEpTION parser...\")\n",
    "parser = InceptionParser()\n",
    "\n",
    "# Parse all documents\n",
    "print(\"\ud83d\udd04 Parsing INCEpTION annotation files...\")\n",
    "documents = parser.parse_directory(INCEPTION_DATA_DIR)\n",
    "\n",
    "# Get parsing summary\n",
    "summary = parser.get_parsing_summary()\n",
    "\n",
    "print(f\"\\n=== PARSING RESULTS ===\")\n",
    "print(f\"\ud83d\udcca Documents successfully parsed: {summary['total_documents_parsed']}\")\n",
    "print(f\"\u26a0\ufe0f Parsing errors: {summary['parsing_errors']}\")\n",
    "print(f\"\ud83c\udfdb\ufe0f Municipalities: {len(summary['municipalities'])}\")\n",
    "print(f\"\ud83d\udccb Municipality list: {', '.join(summary['municipalities'])}\")\n",
    "print(f\"\ud83c\udff7\ufe0f Total entities: {summary['total_entities']:,}\")\n",
    "print(f\"\ud83d\udd17 Total relations: {summary['total_relations']:,}\")\n",
    "\n",
    "if summary['parsing_errors'] > 0:\n",
    "    print(f\"\\n\u26a0\ufe0f Error details:\")\n",
    "    for error in summary['error_details'][:5]:  # Show first 5 errors\n",
    "        print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "print(\"\\n\u2705 Data parsing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataframes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis DataFrames\n",
    "print(\"\ud83d\udd04 Creating analysis DataFrames...\")\n",
    "\n",
    "entities_df = parser.create_entity_dataframe()  # Uses parser.parsed_documents by default\n",
    "relations_df = parser.create_relations_dataframe()  # Uses parser.parsed_documents by default\n",
    "documents_df = parser.create_document_dataframe()  # Uses parser.parsed_documents by default\n",
    "\n",
    "# Filter out empty entity types (annotation artifacts)\n",
    "if not entities_df.empty:\n",
    "    initial_count = len(entities_df)\n",
    "    # Remove entities with empty, null, or whitespace-only labels\n",
    "    entities_df = entities_df[\n",
    "        (entities_df['entity_label'].notna()) & \n",
    "        (entities_df['entity_label'] != '') & \n",
    "        (entities_df['entity_label'].str.strip() != '')\n",
    "    ].copy()\n",
    "    filtered_count = len(entities_df)\n",
    "    removed_count = initial_count - filtered_count\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        print(f\"\ud83e\uddf9 Filtered out {removed_count:,} entities with empty labels (annotation artifacts)\")\n",
    "    else:\n",
    "        print(\"\u2705 No empty entity labels found\")\n",
    "\n",
    "print(f\"\u2705 DataFrames created successfully:\")\n",
    "print(f\"   \u2022 Entities: {len(entities_df):,} rows\")\n",
    "print(f\"   \u2022 Relations: {len(relations_df):,} rows\") \n",
    "print(f\"   \u2022 Documents: {len(documents_df):,} rows\")\n",
    "\n",
    "# Initialize the AnnotationAnalyzer\n",
    "print(\"\\n\ud83d\udd04 Initializing AnnotationAnalyzer...\")\n",
    "analyzer = AnnotationAnalyzer(entities_df, relations_df, documents_df)\n",
    "print(\"\u2705 AnnotationAnalyzer ready for comprehensive analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data structure\n",
    "print(\"\ud83d\udc40 Data Preview\")\n",
    "print(\"\\n=== ENTITIES DATAFRAME ===\")\n",
    "print(entities_df.head())\n",
    "print(f\"\\nColumns: {list(entities_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== RELATIONS DATAFRAME ===\")\n",
    "print(relations_df.head())\n",
    "print(f\"\\nColumns: {list(relations_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== DOCUMENTS DATAFRAME ===\")\n",
    "print(documents_df.head())\n",
    "print(f\"\\nColumns: {list(documents_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus-overview",
   "metadata": {},
   "source": [
    "## 3. Corpus Overview & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corpus-stats-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis\n",
    "print(\"\ud83d\udd2c Running comprehensive statistical analysis...\")\n",
    "\n",
    "try:\n",
    "    comprehensive_analysis = analyzer.run_comprehensive_analysis()\n",
    "    \n",
    "    # Save results using enhanced JSON serialization\n",
    "    results_file = RESULTS_DIR / \"comprehensive_analysis.json\"\n",
    "    \n",
    "    # Enhanced JSON serializer for complex objects\n",
    "    def make_json_serializable(obj):\n",
    "        \"\"\"Recursively convert objects to JSON-serializable format.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            # Handle dictionary keys that might be tuples or other non-string types\n",
    "            new_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                # Convert tuple keys to strings\n",
    "                if isinstance(k, tuple):\n",
    "                    key = str(k)\n",
    "                elif isinstance(k, (int, float, bool)):\n",
    "                    key = str(k)\n",
    "                else:\n",
    "                    key = k\n",
    "                new_dict[key] = make_json_serializable(v)\n",
    "            return new_dict\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        elif hasattr(obj, '__dict__'):\n",
    "            return str(obj)\n",
    "        elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "            return obj\n",
    "        else:\n",
    "            return str(obj)\n",
    "    \n",
    "    # Convert to JSON-serializable format\n",
    "    serializable_analysis = make_json_serializable(comprehensive_analysis)\n",
    "    \n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_analysis, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\u2705 Comprehensive analysis completed successfully!\")\n",
    "    \n",
    "    # Display basic overview\n",
    "    print(\"\\n=== ANALYSIS OVERVIEW ===\")\n",
    "    if 'corpus_statistics' in comprehensive_analysis:\n",
    "        corpus_stats = comprehensive_analysis['corpus_statistics']\n",
    "        if 'corpus_overview' in corpus_stats:\n",
    "            overview = corpus_stats['corpus_overview']\n",
    "            print(f\"\ud83d\udcda Total Documents: {overview.get('total_documents', 0):,}\")\n",
    "            print(f\"\ud83c\udfdb\ufe0f Municipalities: {overview.get('total_municipalities', 0)}\")\n",
    "            print(f\"\ud83d\udcdd Total Entities: {corpus_stats.get('entity_overview', {}).get('total_entities', 0):,}\")\n",
    "            print(f\"\ud83d\udd17 Total Relations: {corpus_stats.get('relation_overview', {}).get('total_relations', 0):,}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Results saved to: {results_file}\")\n",
    "    \n",
    "    # Extract corpus statistics for display\n",
    "    corpus_stats = comprehensive_analysis['corpus_statistics']['corpus_overview']\n",
    "    entity_stats = comprehensive_analysis['corpus_statistics'].get('entity_overview', {})\n",
    "    relation_stats = comprehensive_analysis['corpus_statistics'].get('relation_overview', {})\n",
    "    \n",
    "    # Calculate averages\n",
    "    total_docs = corpus_stats['total_documents']\n",
    "    avg_tokens_per_doc = corpus_stats['total_tokens'] / total_docs if total_docs > 0 else 0\n",
    "    avg_entities_per_doc = entity_stats.get('total_entities', 0) / total_docs if total_docs > 0 else 0\n",
    "    \n",
    "    # Create comprehensive statistics table\n",
    "    stats_data = [\n",
    "        ['\ud83d\udcc4 Total Documents', f\"{corpus_stats['total_documents']:,}\"],\n",
    "        ['\ud83c\udfdb\ufe0f Municipalities', f\"{corpus_stats['total_municipalities']}\"],\n",
    "        ['\ud83d\udccb Documents/Municipality', '20'],\n",
    "        ['\ud83d\udcca Total Text Length', f\"{corpus_stats['total_text_length']:,} characters\"],\n",
    "        ['\ud83d\udd24 Total Tokens', f\"{corpus_stats['total_tokens']:,}\"],\n",
    "        ['\ud83d\udcca Avg Tokens/Document', f\"{round(avg_tokens_per_doc):,}\"],\n",
    "        ['\ud83c\udff7\ufe0f Total Entities', f\"{entity_stats.get('total_entities', 0):,}\"],\n",
    "        ['\ud83d\udccb Avg Entities/Document', f\"{round(avg_entities_per_doc):,}\"],\n",
    "        ['\ud83d\udcdd Entity Types', f\"{entity_stats.get('unique_entity_types', 0)}\"],\n",
    "        ['\ud83d\udd17 Total Relations', f\"{relation_stats.get('total_relations', 0):,}\"],\n",
    "        ['\ud83d\udcc8 Documents with Entities', f\"{entity_stats.get('documents_with_entities', 0):,}\"],\n",
    "        ['\ud83d\udcca Entity Coverage', f\"{entity_stats.get('entity_coverage', 0):.1%}\"]\n",
    "    ]\n",
    "\n",
    "    # Display as formatted table\n",
    "    stats_df = pd.DataFrame(stats_data, columns=['Metric', 'Value'])\n",
    "    print(\"\\n=== CORPUS OVERVIEW STATISTICS ===\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "\n",
    "    # Save statistics table\n",
    "    stats_df.to_csv(STATISTICS_DIR / 'corpus_overview_table.csv', index=False)\n",
    "    print(f\"\\n\ud83d\udcbe Statistics table saved to {STATISTICS_DIR / 'corpus_overview_table.csv'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error during analysis: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39273f7a",
   "metadata": {},
   "source": [
    "## 4. Entity Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity type distribution analysis\n",
    "if not entities_df.empty:\n",
    "    entity_type_counts = entities_df['entity_label'].value_counts()\n",
    "    entity_type_percentages = (entity_type_counts / entity_type_counts.sum() * 100).round(2)\n",
    "    \n",
    "    print(\"=== ENTITY TYPE DISTRIBUTION ===\")\n",
    "    for entity_type, count in entity_type_counts.items():\n",
    "        percentage = entity_type_percentages[entity_type]\n",
    "        print(f\"{entity_type}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Define consistent color palette (Set3) - vibrant but not overwhelming\n",
    "    import plotly.colors\n",
    "    entity_types = list(entity_type_counts.index)\n",
    "    color_palette = plotly.colors.qualitative.Set3[:len(entity_types)]\n",
    "    \n",
    "    # Create combined visualization\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Entity Type Frequencies', 'Entity Type Distribution'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    "    )\n",
    "    \n",
    "    # Bar chart with balanced colors\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=entity_type_counts.index, y=entity_type_counts.values, name='Count',\n",
    "               marker_color=color_palette),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Pie chart with balanced colors\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=entity_type_counts.index, values=entity_type_counts.values, name='Distribution',\n",
    "               marker_colors=color_palette),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"\ud83c\udff7\ufe0f Entity Type Analysis\",\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save figure\n",
    "    # Save figure using Colab-friendly utility\n",
    "    save_figure(\n",
    "        fig, \n",
    "        'entity_type_analysis', \n",
    "        FIGURES_DIR,\n",
    "        save_html=SAVE_HTML, \n",
    "        save_image=SAVE_IMAGES, \n",
    "        width=1200, \n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No entities found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ec5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of entities and relations by municipality\n",
    "print(\"=== ENTITIES AND RELATIONS BY MUNICIPALITY ===\")\n",
    "\n",
    "if not entities_df.empty:\n",
    "    # Group entities by municipality\n",
    "    entities_by_municipality = entities_df.groupby('municipality').agg({\n",
    "        'entity_id': 'count',  # Count of entities\n",
    "        'entity_label': lambda x: x.value_counts().to_dict()  # Entity type distribution per municipality\n",
    "    }).rename(columns={'entity_id': 'total_entities'})\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca ENTITY COUNTS BY MUNICIPALITY:\")\n",
    "    entities_by_municipality_sorted = entities_by_municipality.sort_values('total_entities', ascending=False)\n",
    "    \n",
    "    for municipality, row in entities_by_municipality_sorted.iterrows():\n",
    "        total_entities = row['total_entities']\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f {municipality}: {total_entities:,} entities\")\n",
    "        \n",
    "        # Show entity type breakdown for this municipality\n",
    "        entity_types = row['entity_label']\n",
    "        for entity_type, count in sorted(entity_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total_entities) * 100\n",
    "            print(f\"   \u2022 {entity_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if not relations_df.empty:\n",
    "    # Group relations by municipality\n",
    "    relations_by_municipality = relations_df.groupby('municipality').agg({\n",
    "        'relation_id': 'count',  # Count of relations\n",
    "        'relation_type': lambda x: x.value_counts().to_dict()  # Relation type distribution per municipality\n",
    "    }).rename(columns={'relation_id': 'total_relations'})\n",
    "    \n",
    "    print(\"\\n\\n\ud83d\udd17 RELATION COUNTS BY MUNICIPALITY:\")\n",
    "    relations_by_municipality_sorted = relations_by_municipality.sort_values('total_relations', ascending=False)\n",
    "    \n",
    "    for municipality, row in relations_by_municipality_sorted.iterrows():\n",
    "        total_relations = row['total_relations']\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f {municipality}: {total_relations:,} relations\")\n",
    "        \n",
    "        # Show relation type breakdown for this municipality\n",
    "        relation_types = row['relation_type']\n",
    "        for relation_type, count in sorted(relation_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / total_relations) * 100\n",
    "            print(f\"   \u2022 {relation_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create visualization combining entities and relations by municipality\n",
    "if not entities_df.empty or not relations_df.empty:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Entities by Municipality', 'Relations by Municipality',\n",
    "            'Entity Density (per Document)', 'Relation Density (per Document)'\n",
    "        ),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Entities by municipality\n",
    "    if not entities_df.empty:\n",
    "        entity_counts = entities_by_municipality_sorted['total_entities']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=entity_counts.index,\n",
    "                y=entity_counts.values,\n",
    "                name='Entities',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Calculate entity density (entities per document)\n",
    "        if not documents_df.empty:\n",
    "            doc_counts_by_municipality = documents_df['municipality'].value_counts()\n",
    "            entity_density = entity_counts / doc_counts_by_municipality.reindex(entity_counts.index)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=entity_density.index,\n",
    "                    y=entity_density.values,\n",
    "                    name='Entity Density',\n",
    "                    marker_color='lightgreen'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Relations by municipality\n",
    "    if not relations_df.empty:\n",
    "        relation_counts = relations_by_municipality_sorted['total_relations']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=relation_counts.index,\n",
    "                y=relation_counts.values,\n",
    "                name='Relations',\n",
    "                marker_color='lightcoral'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Calculate relation density (relations per document)\n",
    "        if not documents_df.empty:\n",
    "            doc_counts_by_municipality = documents_df['municipality'].value_counts()\n",
    "            relation_density = relation_counts / doc_counts_by_municipality.reindex(relation_counts.index)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=relation_density.index,\n",
    "                    y=relation_density.values,\n",
    "                    name='Relation Density',\n",
    "                    marker_color='lightyellow'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"\ud83c\udfdb\ufe0f Entities and Relations Distribution by Municipality\",\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    fig.update_xaxes(tickangle=-45)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save figure\n",
    "    # Save figure using Colab-friendly utility\n",
    "    save_figure(\n",
    "        fig, \n",
    "        'entities_relations_by_municipality', \n",
    "        FIGURES_DIR,\n",
    "        save_html=SAVE_HTML, \n",
    "        save_image=SAVE_IMAGES, \n",
    "        width=1400, \n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcbe Municipality analysis visualization saved\")\n",
    "\n",
    "# Create stacked bar chart for entity types by municipality\n",
    "if not entities_df.empty:\n",
    "    print(\"\\n\ud83d\udcca Creating stacked bar chart for entity types by municipality...\")\n",
    "    \n",
    "    # Create a cross-tabulation of municipalities and entity types\n",
    "    entity_muni_cross = pd.crosstab(entities_df['municipality'], entities_df['entity_label'])\n",
    "    \n",
    "    # Sort municipalities by total entity count for consistent ordering\n",
    "    entity_muni_cross = entity_muni_cross.loc[entities_by_municipality_sorted.index]\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    fig_entity_types = go.Figure()\n",
    "    \n",
    "    # Get distinct colors for each entity type\n",
    "    entity_types = entity_muni_cross.columns.tolist()\n",
    "    colors = px.colors.qualitative.Set3[:len(entity_types)]\n",
    "    \n",
    "    # Add traces for each entity type\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        fig_entity_types.add_trace(\n",
    "            go.Bar(\n",
    "                name=entity_type,\n",
    "                x=entity_muni_cross.index,\n",
    "                y=entity_muni_cross[entity_type],\n",
    "                marker_color=colors[i % len(colors)]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig_entity_types.update_layout(\n",
    "        title='\ud83c\udff7\ufe0f Entity Types Distribution by Municipality (Stacked)',\n",
    "        xaxis_title='Municipality',\n",
    "        yaxis_title='Number of Entities',\n",
    "        barmode='stack',\n",
    "        height=600,\n",
    "        xaxis_tickangle=-45,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig_entity_types.show()\n",
    "    \n",
    "    # Save the stacked bar chart\n",
    "    save_figure(\n",
    "        fig_entity_types, \n",
    "        'entity_types_by_municipality_stacked', \n",
    "        FIGURES_DIR,\n",
    "        save_html=SAVE_HTML, \n",
    "        save_image=SAVE_IMAGES, \n",
    "        width=1200, \n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    print(f\"\ud83d\udcbe Entity types stacked bar chart saved\")\n",
    "    \n",
    "    # Print summary of entity type distribution\n",
    "    print(f\"\\n\ud83d\udcc8 ENTITY TYPE DISTRIBUTION SUMMARY:\")\n",
    "    total_by_type = entity_muni_cross.sum().sort_values(ascending=False)\n",
    "    print(f\"\u2022 Total entity types found: {len(entity_types)}\")\n",
    "    print(f\"\u2022 Most common entity type: {total_by_type.index[0]} ({total_by_type.iloc[0]:,} entities)\")\n",
    "    print(f\"\u2022 Least common entity type: {total_by_type.index[-1]} ({total_by_type.iloc[-1]:,} entities)\")\n",
    "    \n",
    "    # Show top entity types across all municipalities\n",
    "    print(f\"\\n\ud83c\udfc6 TOP ENTITY TYPES (across all municipalities):\")\n",
    "    for entity_type, count in total_by_type.head(10).items():\n",
    "        percentage = (count / total_by_type.sum()) * 100\n",
    "        print(f\"   \u2022 {entity_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Summary statistics\n",
    "if not entities_df.empty and not relations_df.empty:\n",
    "    print(f\"\\n\ud83d\udcc8 SUMMARY STATISTICS:\")\n",
    "    print(f\"\u2022 Total municipalities with entities: {len(entities_by_municipality)}\")\n",
    "    print(f\"\u2022 Total municipalities with relations: {len(relations_by_municipality)}\")\n",
    "    print(f\"\u2022 Average entities per municipality: {entities_by_municipality['total_entities'].mean():.1f}\")\n",
    "    print(f\"\u2022 Average relations per municipality: {relations_by_municipality['total_relations'].mean():.1f}\")\n",
    "    \n",
    "    # Find municipalities with highest/lowest counts\n",
    "    max_entities_mun = entities_by_municipality['total_entities'].idxmax()\n",
    "    min_entities_mun = entities_by_municipality['total_entities'].idxmin()\n",
    "    max_relations_mun = relations_by_municipality['total_relations'].idxmax()\n",
    "    min_relations_mun = relations_by_municipality['total_relations'].idxmin()\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfc6 Municipality with most entities: {max_entities_mun} ({entities_by_municipality.loc[max_entities_mun, 'total_entities']:,})\")\n",
    "    print(f\"\ud83c\udfc6 Municipality with most relations: {max_relations_mun} ({relations_by_municipality.loc[max_relations_mun, 'total_relations']:,})\")\n",
    "    print(f\"\ud83d\udcc9 Municipality with fewest entities: {min_entities_mun} ({entities_by_municipality.loc[min_entities_mun, 'total_entities']:,})\")\n",
    "    print(f\"\ud83d\udcc9 Municipality with fewest relations: {min_relations_mun} ({relations_by_municipality.loc[min_relations_mun, 'total_relations']:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity length and characteristics analysis\n",
    "if not entities_df.empty:\n",
    "    # Entity length distribution\n",
    "    fig_length = px.histogram(\n",
    "        entities_df,\n",
    "        x='length',\n",
    "        nbins=30,\n",
    "        title='\ud83d\udccf Entity Length Distribution (Characters)',\n",
    "        labels={'length': 'Entity Length (characters)', 'count': 'Frequency'},\n",
    "        marginal='box'  # Add box plot\n",
    "    )\n",
    "    \n",
    "    fig_length.update_layout(height=500)\n",
    "    fig_length.show()\n",
    "    \n",
    "    # Token count distribution\n",
    "    fig_tokens = px.histogram(\n",
    "        entities_df,\n",
    "        x='token_count',\n",
    "        nbins=20,\n",
    "        title='\ud83d\udd24 Entity Token Count Distribution',\n",
    "        labels={'token_count': 'Token Count', 'count': 'Frequency'},\n",
    "        marginal='violin'  # Add violin plot\n",
    "    )\n",
    "    \n",
    "    fig_tokens.update_layout(height=500)\n",
    "    fig_tokens.show()\n",
    "    \n",
    "    # Length by entity type\n",
    "    fig_length_by_type = px.box(\n",
    "        entities_df,\n",
    "        x='entity_label',\n",
    "        y='length',\n",
    "        title='\ud83d\udcca Entity Length Distribution by Type',\n",
    "        labels={'entity_label': 'Entity Type', 'length': 'Length (characters)'},\n",
    "        points='outliers'\n",
    "    )\n",
    "    \n",
    "    fig_length_by_type.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_length_by_type.show()\n",
    "    \n",
    "    # Save figures\n",
    "    fig_length.write_html(FIGURES_DIR / 'entity_length_distribution.html')\n",
    "    fig_tokens.write_html(FIGURES_DIR / 'entity_token_distribution.html')\n",
    "    fig_length_by_type.write_html(FIGURES_DIR / 'entity_length_by_type.html')\n",
    "    \n",
    "    print(\"\ud83d\udcbe Entity characteristics visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c25b8",
   "metadata": {},
   "source": [
    "### 4.1 Statistical Significance Testing\n",
    "\n",
    "This sub-section presents statistical tests to determine whether the observed differences in entity type distributions and characteristics across municipalities are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f39624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance Testing for Entity Type Analysis\n",
    "print(\"=== STATISTICAL SIGNIFICANCE TESTS FOR ENTITY TYPE ANALYSIS ===\")\n",
    "\n",
    "# Import required libraries for statistical testing\n",
    "from scipy.stats import chi2_contingency, kruskal, f_oneway\n",
    "import numpy as np\n",
    "\n",
    "# 1. Chi-square test for entity type distribution across municipalities\n",
    "if not entities_df.empty and len(entities_df['municipality'].unique()) > 1:\n",
    "    print(\"\\n1. \ud83e\uddee Chi-square Test for Entity Type Distribution Across Municipalities\")\n",
    "\n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(entities_df['municipality'], entities_df['entity_label'])\n",
    "\n",
    "    if contingency_table.shape[0] > 1 and contingency_table.shape[1] > 1:\n",
    "        chi2, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Cram\u00e9r's V (effect size for chi-square)\n",
    "        n = contingency_table.sum().sum()\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "        print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "        if p_val == 0.0:\n",
    "            print(f\"   p-value: < 1e-323 (reported as 0.0 due to numerical precision)\")\n",
    "            print(f\"   \ud83d\udcdd Note: p-value = 0 indicates extremely strong evidence against null hypothesis\")\n",
    "        else:\n",
    "            print(f\"   p-value: {p_val:.6f}\")\n",
    "        print(f\"   Degrees of freedom: {dof}\")\n",
    "        print(f\"   Cram\u00e9r's V (effect size): {cramers_v:.4f}\")\n",
    "\n",
    "        if p_val < 0.05 or p_val == 0.0:\n",
    "            print(f\"   \u2705 SIGNIFICANT: Entity type distributions differ significantly across municipalities\")\n",
    "            if cramers_v < 0.1:\n",
    "                print(\"   \ud83d\udcca Effect size: Very small\")\n",
    "            elif cramers_v < 0.3:\n",
    "                print(\"   \ud83d\udcca Effect size: Small\")\n",
    "            elif cramers_v < 0.5:\n",
    "                print(\"   \ud83d\udcca Effect size: Medium\")\n",
    "            else:\n",
    "                print(\"   \ud83d\udcca Effect size: Large\")\n",
    "        else:\n",
    "            print(f\"   \u274c NOT SIGNIFICANT: No significant differences in entity type distributions\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Insufficient data for chi-square test\")\n",
    "\n",
    "# 2. ANOVA/Kruskal-Wallis test for entity counts by municipality\n",
    "if not entities_df.empty and len(entities_df['municipality'].unique()) > 1:\n",
    "    print(\"\\n2. \ud83d\udcca Test for Entity Count Differences Across Municipalities\")\n",
    "    \n",
    "    # Group entity counts by municipality\n",
    "    entity_counts_by_muni = entities_df.groupby('municipality').size()\n",
    "    \n",
    "    if len(entity_counts_by_muni) > 2:\n",
    "        # Use Kruskal-Wallis as it's more robust for count data\n",
    "        try:\n",
    "            h_stat, p_kruskal = kruskal(*[entities_df[entities_df['municipality'] == muni].shape[0] \n",
    "                                        for muni in entity_counts_by_muni.index if \n",
    "                                        entities_df[entities_df['municipality'] == muni].shape[0] > 0])\n",
    "            \n",
    "            if not np.isnan(h_stat):\n",
    "                print(f\"   Kruskal-Wallis H-statistic: {h_stat:.4f}\")\n",
    "                print(f\"   p-value: {p_kruskal:.6f}\")\n",
    "                \n",
    "                if p_kruskal < 0.05:\n",
    "                    print(\"   \u2705 SIGNIFICANT (Kruskal-Wallis): Entity counts differ significantly across municipalities\")\n",
    "                else:\n",
    "                    print(\"   \u274c NOT SIGNIFICANT (Kruskal-Wallis): No significant differences in entity counts\")\n",
    "            else:\n",
    "                print(\"   \u26a0\ufe0f Unable to compute test statistic\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f Error in statistical test: {e}\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Need at least 3 municipalities for this test\")\n",
    "# 3. Kruskal-Wallis test for entity length differences by type\n",
    "if not entities_df.empty and len(entities_df['entity_label'].unique()) > 1:\n",
    "    print(\"\\n3. \ud83d\udccf Kruskal-Wallis Test for Entity Length Differences by Type\")\n",
    "\n",
    "    # Group lengths by entity type\n",
    "    length_groups = [group['length'].values for name, group in entities_df.groupby('entity_label')]\n",
    "\n",
    "    if len(length_groups) > 1 and all(len(group) > 0 for group in length_groups):\n",
    "        h_stat, p_kw = kruskal(*length_groups)\n",
    "\n",
    "        print(f\"   Kruskal-Wallis H-statistic: {h_stat:.4f}\")\n",
    "        print(f\"   p-value: {p_kw:.6f}\")\n",
    "\n",
    "        if p_kw < 0.05:\n",
    "            print(\"   \u2705 SIGNIFICANT: Entity lengths differ significantly by entity type\")\n",
    "        else:\n",
    "            print(\"   \u274c NOT SIGNIFICANT: No significant differences in entity lengths by type\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Insufficient data for length comparison test\")\n",
    "\n",
    "# 4. Test for entity density differences\n",
    "if not entities_df.empty and not documents_df.empty:\n",
    "    print(\"\\n4. \ud83d\udcc8 Test for Entity Density Differences Across Municipalities\")\n",
    "    \n",
    "    # Calculate entity density (entities per document) by municipality\n",
    "    doc_counts = documents_df['municipality'].value_counts()\n",
    "    entity_counts = entities_df['municipality'].value_counts()\n",
    "    density = entity_counts / doc_counts\n",
    "    \n",
    "    if len(density) > 2:\n",
    "        # Use Kruskal-Wallis for density data\n",
    "        try:\n",
    "            h_stat, p_kruskal = kruskal(*[entities_df[entities_df['municipality'] == muni].shape[0] / doc_counts[muni]\n",
    "                                        for muni in density.index if doc_counts[muni] > 0])\n",
    "            \n",
    "            if not np.isnan(h_stat):\n",
    "                print(f\"   Kruskal-Wallis H-statistic: {h_stat:.4f}\")\n",
    "                print(f\"   p-value: {p_kruskal:.6f}\")\n",
    "                \n",
    "                if p_kruskal < 0.05:\n",
    "                    print(\"   \u2705 SIGNIFICANT (Kruskal-Wallis): Entity density differs significantly across municipalities\")\n",
    "                else:\n",
    "                    print(\"   \u274c NOT SIGNIFICANT (Kruskal-Wallis): No significant differences in entity density\")\n",
    "            else:\n",
    "                print(\"   \u26a0\ufe0f Unable to compute test statistic\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f Error in statistical test: {e}\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Need at least 3 municipalities for density test\")\n",
    "print(\"\\n\ud83d\udccb SUMMARY OF STATISTICAL TESTS:\")\n",
    "print(\"   - Chi-square test: Tests if entity type proportions differ by municipality\")\n",
    "print(\"   - ANOVA/Kruskal-Wallis: Tests if entity counts/density differ by municipality\")\n",
    "print(\"   - Kruskal-Wallis: Tests if entity lengths differ by entity type\")\n",
    "print(\"   - p < 0.05 indicates statistically significant differences\")\n",
    "print(\"   - Effect sizes help interpret practical significance of findings\")\n",
    "\n",
    "print(\"\\n\ud83d\udcbe Statistical test results completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posicionamento-section",
   "metadata": {},
   "source": [
    "## 5. Posicionamento Analysis (Voting Behavior) - Individual Time Pattern Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posicionamento-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posicionamento analysis\n",
    "if not relations_df.empty and 'posicionamento' in relations_df.columns:\n",
    "    # Filter relations with posicionamento data\n",
    "    pos_data = relations_df.dropna(subset=['posicionamento'])\n",
    "    \n",
    "    if not pos_data.empty:\n",
    "        print(\"=== POSICIONAMENTO ANALYSIS ===\")\n",
    "        \n",
    "        # Overall posicionamento distribution\n",
    "        pos_counts = pos_data['posicionamento'].value_counts()\n",
    "        pos_percentages = (pos_counts / pos_counts.sum() * 100).round(2)\n",
    "        \n",
    "        print(\"\\nPositioning Distribution:\")\n",
    "        for pos, count in pos_counts.items():\n",
    "            percentage = pos_percentages[pos]\n",
    "            print(f\"  {pos}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig_pos = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Posicionamento Frequencies', 'Posicionamento Distribution'),\n",
    "            specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    "        )\n",
    "        \n",
    "        # Use consistent color palette (Set3)\n",
    "        colors = px.colors.qualitative.Set3[:len(pos_counts)]\n",
    "        fig_pos.add_trace(\n",
    "            go.Bar(x=pos_counts.index, y=pos_counts.values, \n",
    "                   marker_color=colors, name='Count'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Pie chart\n",
    "        fig_pos.add_trace(\n",
    "            go.Pie(labels=pos_counts.index, values=pos_counts.values, \n",
    "                   marker_colors=colors, name='Distribution'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig_pos.update_layout(\n",
    "            title_text=\"\ud83d\uddf3\ufe0f Posicionamento (Voting Position) Analysis\",\n",
    "            height=500,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig_pos.show()\n",
    "        \n",
    "        # Save figure\n",
    "        # Save figure using Colab-friendly utility\n",
    "        save_figure(\n",
    "            fig_pos, \n",
    "            'posicionamento_analysis', \n",
    "            FIGURES_DIR,\n",
    "            save_html=SAVE_HTML, \n",
    "            save_image=SAVE_IMAGES, \n",
    "            width=1200, \n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No posicionamento data found\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No relations or posicionamento column found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posicionamento-by-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posicionamento by municipality analysis\n",
    "if not relations_df.empty and 'posicionamento' in relations_df.columns:\n",
    "    pos_data = relations_df.dropna(subset=['posicionamento'])\n",
    "    \n",
    "    if not pos_data.empty:\n",
    "        # Cross-tabulation\n",
    "        pos_muni_cross = pd.crosstab(pos_data['municipality'], pos_data['posicionamento'])\n",
    "        \n",
    "        # Normalize by municipality (percentage within each municipality)\n",
    "        pos_muni_pct = pos_muni_cross.div(pos_muni_cross.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        fig_pos_muni = go.Figure()\n",
    "        \n",
    "        colors = px.colors.qualitative.Set3\n",
    "        for i, pos_type in enumerate(pos_muni_cross.columns):\n",
    "            fig_pos_muni.add_trace(go.Bar(\n",
    "                name=pos_type,\n",
    "                x=pos_muni_cross.index,\n",
    "                y=pos_muni_cross[pos_type],\n",
    "                marker_color=colors[i % len(colors)]\n",
    "            ))\n",
    "        \n",
    "        fig_pos_muni.update_layout(\n",
    "            title='\ud83c\udfdb\ufe0f Posicionamento Distribution by Municipality',\n",
    "            xaxis_title='Municipality',\n",
    "            yaxis_title='Number of Positions',\n",
    "            barmode='stack',\n",
    "            xaxis_tickangle=-45,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig_pos_muni.show()\n",
    "        \n",
    "        # Create percentage heatmap\n",
    "        fig_pos_heatmap = px.imshow(\n",
    "            pos_muni_pct.values,\n",
    "            x=pos_muni_pct.columns,\n",
    "            y=pos_muni_pct.index,\n",
    "            title='\ud83d\udcca Posicionamento Patterns by Municipality (%)',\n",
    "            labels={'x': 'Position Type', 'y': 'Municipality', 'color': 'Percentage'},\n",
    "            aspect='auto',\n",
    "            color_continuous_scale='RdYlBu'\n",
    "        )\n",
    "        \n",
    "        fig_pos_heatmap.update_layout(height=500)\n",
    "        fig_pos_heatmap.show()\n",
    "        \n",
    "        # Save figures\n",
    "        fig_pos_muni.write_html(FIGURES_DIR / 'posicionamento_by_municipality.html')\n",
    "        fig_pos_heatmap.write_html(FIGURES_DIR / 'posicionamento_patterns_heatmap.html')\n",
    "        \n",
    "        print(\"\ud83d\udcbe Posicionamento by municipality analysis saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ee7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the posicionamento analysis visualization\n",
    "print(\"=== POSICIONAMENTO ANALYSIS ===\")\n",
    "\n",
    "# Get posicionamento data\n",
    "posicionamento_data = entities_df[entities_df['posicionamento'].notna()]\n",
    "\n",
    "if not posicionamento_data.empty:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.colors\n",
    "    \n",
    "    print(\"\ud83d\uddf3\ufe0f Posicionamento (Voting Position) Analysis:\")\n",
    "    \n",
    "    # Posicionamento distribution\n",
    "    pos_counts = posicionamento_data['posicionamento'].value_counts()\n",
    "    total_pos = len(posicionamento_data)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Position Distribution ({total_pos:,} total entities):\")\n",
    "    for pos_type, count in pos_counts.items():\n",
    "        pct = (count / total_pos) * 100\n",
    "        print(f\"   {pos_type}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Define consistent color palette for all charts\n",
    "    position_types = list(pos_counts.keys())\n",
    "    color_palette = plotly.colors.qualitative.Set3[:len(position_types)]\n",
    "    color_map = dict(zip(position_types, color_palette))\n",
    "    \n",
    "    # 1. Position Distribution - Individual Pie Chart\n",
    "    print(\"\\n\ud83d\udcca Position Distribution:\")\n",
    "    fig_pie = go.Figure(data=[\n",
    "        go.Pie(\n",
    "            labels=list(pos_counts.keys()),\n",
    "            values=list(pos_counts.values),\n",
    "            marker=dict(colors=[color_map[pos] for pos in pos_counts.keys()]),\n",
    "            textinfo='label+percent',\n",
    "            hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent}<extra></extra>'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_pie.update_layout(\n",
    "        title=\"Position Distribution\",\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig_pie.show()\n",
    "    \n",
    "    # Extract temporal patterns\n",
    "    posicionamento_data_fixed = posicionamento_data.copy()\n",
    "    \n",
    "    def extract_date_info(filename):\n",
    "        try:\n",
    "            # Extract date from filename pattern like \"Municipality_cm_XXX_YYYY-MM-DD.json\"\n",
    "            parts = filename.split('_')\n",
    "            for part in parts:\n",
    "                if len(part) >= 10 and '-' in part:  # Date part\n",
    "                    date_part = part.split('.')[0]  # Remove .json\n",
    "                    if len(date_part) >= 10:\n",
    "                        return date_part\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    posicionamento_data_fixed['date_str'] = posicionamento_data_fixed['filename'].apply(extract_date_info)\n",
    "    posicionamento_data_fixed['date'] = pd.to_datetime(posicionamento_data_fixed['date_str'], errors='coerce')\n",
    "    posicionamento_data_fixed = posicionamento_data_fixed.dropna(subset=['date'])\n",
    "    \n",
    "    # 2. Monthly Patterns - Individual Bar Chart\n",
    "    print(\"\\n\ud83d\udcc5 Monthly Patterns:\")\n",
    "    posicionamento_data_fixed['month'] = posicionamento_data_fixed['date'].dt.to_period('M')\n",
    "    monthly_patterns = posicionamento_data_fixed.groupby(['month', 'posicionamento']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig_monthly = go.Figure()\n",
    "    for pos_type in monthly_patterns.columns:\n",
    "        fig_monthly.add_trace(\n",
    "            go.Bar(\n",
    "                x=[str(m) for m in monthly_patterns.index],\n",
    "                y=list(monthly_patterns[pos_type]),\n",
    "                name=pos_type,\n",
    "                marker_color=color_map.get(pos_type, 'gray'),\n",
    "                hovertemplate='<b>%{fullData.name}</b><br>Month: %{x}<br>Count: %{y}<extra></extra>'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig_monthly.update_layout(\n",
    "        title=\"Monthly Position Patterns\",\n",
    "        xaxis_title=\"Month\",\n",
    "        yaxis_title=\"Number of Positions\",\n",
    "        height=600,\n",
    "        barmode='stack',\n",
    "        xaxis_tickangle=-45,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig_monthly.show()\n",
    "    \n",
    "    # 3. Yearly Evolution - Individual Line Chart\n",
    "    print(\"\\n\ud83d\udcc8 Yearly Evolution:\")\n",
    "    posicionamento_data_fixed['year'] = posicionamento_data_fixed['date'].dt.year\n",
    "    yearly_patterns = posicionamento_data_fixed.groupby(['year', 'posicionamento']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig_yearly = go.Figure()\n",
    "    for pos_type in yearly_patterns.columns:\n",
    "        fig_yearly.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(yearly_patterns.index),\n",
    "                y=list(yearly_patterns[pos_type]),\n",
    "                mode='lines+markers',\n",
    "                name=pos_type,\n",
    "                line=dict(color=color_map.get(pos_type, 'gray'), width=3),\n",
    "                marker=dict(size=8),\n",
    "                hovertemplate='<b>%{fullData.name}</b><br>Year: %{x}<br>Count: %{y}<extra></extra>'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig_yearly.update_layout(\n",
    "        title=\"Yearly Position Evolution\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Number of Positions\",\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig_yearly.show()\n",
    "    \n",
    "    # Save individual figures\n",
    "    if 'FIGURES_DIR' in globals():\n",
    "        fig_pie.write_html(FIGURES_DIR / 'posicionamento_distribution.html')\n",
    "        fig_monthly.write_html(FIGURES_DIR / 'posicionamento_monthly_patterns.html')\n",
    "        fig_yearly.write_html(FIGURES_DIR / 'posicionamento_yearly_evolution.html')\n",
    "        print(\"\ud83d\udcbe Individual posicionamento plots saved\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc5 Temporal Analysis Summary:\")\n",
    "    print(f\"   - Date range: {posicionamento_data_fixed['date'].min()} to {posicionamento_data_fixed['date'].max()}\")\n",
    "    print(f\"   - Total dated entries: {len(posicionamento_data_fixed)}\")\n",
    "    \n",
    "    # Monthly summary\n",
    "    print(f\"\\n\ud83d\udcca Monthly Patterns:\")\n",
    "    for month in sorted(monthly_patterns.index):\n",
    "        total = monthly_patterns.loc[month].sum()\n",
    "        print(f\"   - {month}: {total} position mentions\")\n",
    "    \n",
    "    # Yearly summary\n",
    "    print(f\"\\n\ud83d\udcc8 Yearly Evolution:\")\n",
    "    for year in sorted(yearly_patterns.index):\n",
    "        total = yearly_patterns.loc[year].sum()\n",
    "        print(f\"   - {year}: {total} position mentions\")\n",
    "        \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No posicionamento data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c80359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votations Analysis: Unanimity vs Majority Analysis\n",
    "print(\"=== VOTATIONS ANALYSIS: UNANIMITY VS MAJORITY ===\")\n",
    "\n",
    "# Get votation results from relations (not entities)\n",
    "# Each votacao has its result stored in a \"resultado\" relation\n",
    "votations_relations = relations_df[relations_df['relation_label'] == 'resultado'].copy()\n",
    "\n",
    "if not votations_relations.empty:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    print(\"\ud83d\uddf3\ufe0f Votation Results Analysis:\")\n",
    "    \n",
    "    # Clean and analyze votation results\n",
    "    votations_relations['resultado_clean'] = votations_relations['resultado'].str.strip().str.lower()\n",
    "    \n",
    "    # Map results to categories\n",
    "    def categorize_result(result):\n",
    "        if pd.isna(result) or result == '':\n",
    "            return 'Unknown'\n",
    "        result = result.lower().strip()\n",
    "        if 'unanimidade' in result or 'unanime' in result:\n",
    "            return 'Unanimity'\n",
    "        elif 'maioria' in result or 'majority' in result:\n",
    "            return 'Majority'\n",
    "        elif 'favor' in result:\n",
    "            return 'In Favor'\n",
    "        elif 'contra' in result:\n",
    "            return 'Against'\n",
    "        else:\n",
    "            return f'Other: {result.title()}'\n",
    "    \n",
    "    votations_relations['result_category'] = votations_relations['resultado_clean'].apply(categorize_result)\n",
    "    \n",
    "    # Analyze votation results\n",
    "    result_counts = votations_relations['result_category'].value_counts()\n",
    "    total_votations = len(votations_relations)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Votation Results ({total_votations:,} total votations):\")\n",
    "    for result_type, count in result_counts.items():\n",
    "        pct = (count / total_votations) * 100\n",
    "        print(f\"   {result_type}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Get VOTACAO entities (entities with posicionamento = \"Vota\u00e7\u00e3o\")\n",
    "    votacao_entities = entities_df[entities_df['posicionamento'] == 'Vota\u00e7\u00e3o']\n",
    "    total_votacao_entities = len(votacao_entities)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd0d Data Coverage Analysis:\")\n",
    "    print(f\"   Total VOTACAO entities found: {total_votacao_entities:,}\")\n",
    "    print(f\"   Total contabilizacao relations with results: {total_votations:,}\")\n",
    "    if total_votacao_entities > 0:\n",
    "        coverage_pct = total_votations/total_votacao_entities*100\n",
    "        print(f\"   Coverage: {coverage_pct:.1f}% of votacao entities have result details\")\n",
    "    else:\n",
    "        print(f\"   Coverage: Cannot calculate (no VOTACAO entities found)\")\n",
    "    \n",
    "    # Votations by municipality with totals\n",
    "    votations_by_muni = votations_relations.groupby(['municipality', 'result_category']).size().unstack(fill_value=0)\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfdb\ufe0f Votation Results by Municipality:\")\n",
    "    for municipality in votations_by_muni.index:\n",
    "        total_contabilizacao = votations_by_muni.loc[municipality].sum()\n",
    "        \n",
    "        # Get municipality-specific VOTACAO entities\n",
    "        muni_votacao_entities = votacao_entities[votacao_entities['municipality'] == municipality]\n",
    "        muni_total_votacao = len(muni_votacao_entities)\n",
    "        \n",
    "        # Get entity IDs that have contabilizacao relations (CORRECTLY!)\n",
    "        muni_relations = votations_relations[votations_relations['municipality'] == municipality]\n",
    "        muni_contabilizacao_entity_ids = set()\n",
    "        if not muni_relations.empty:\n",
    "            dependent_ids = set(muni_relations['dependent_id'].dropna().astype(int))\n",
    "            governor_ids = set(muni_relations['governor_id'].dropna().astype(int))\n",
    "            muni_contabilizacao_entity_ids = dependent_ids.union(governor_ids)\n",
    "        \n",
    "        # Find VOTACAO entities that are actually involved in relations\n",
    "        muni_votacao_with_relations = muni_votacao_entities[\n",
    "            muni_votacao_entities['entity_id'].isin(muni_contabilizacao_entity_ids)\n",
    "        ]\n",
    "        muni_votacao_without_relations = muni_votacao_entities[\n",
    "            ~muni_votacao_entities['entity_id'].isin(muni_contabilizacao_entity_ids)\n",
    "        ]\n",
    "        \n",
    "        actual_coverage_count = len(muni_votacao_with_relations)\n",
    "        actual_gap = len(muni_votacao_without_relations)\n",
    "        \n",
    "        print(f\"   - {municipality}:\")\n",
    "        print(f\"     \u2022 Total VOTACAO entities: {muni_total_votacao}\")\n",
    "        print(f\"     \u2022 Contabilizacao relations: {total_contabilizacao}\")\n",
    "        print(f\"     \u2022 VOTACAO entities WITH contabilizacao: {actual_coverage_count}\")\n",
    "        if muni_total_votacao > 0:\n",
    "            coverage_pct = actual_coverage_count/muni_total_votacao*100\n",
    "            print(f\"     \u2022 Coverage: {coverage_pct:.1f}%\")\n",
    "            print(f\"     \u2022 VOTACAO entities without contabilizacao: {actual_gap}\")\n",
    "        else:\n",
    "            print(f\"     \u2022 Coverage: Cannot calculate (no VOTACAO entities)\")\n",
    "        \n",
    "        # Show breakdown by result type\n",
    "        for result_type in votations_by_muni.columns:\n",
    "            count = votations_by_muni.loc[municipality, result_type]\n",
    "            if count > 0:\n",
    "                pct = (count / total_contabilizacao) * 100\n",
    "                print(f\"     \u2022 {result_type}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Diagnostic: Show examples of VOTACAO entities without contabilizacao for Campomaior\n",
    "    print(f\"\\n\ud83d\udd0d Diagnostic Analysis - Campomaior VOTACAO entities without contabilizacao:\")\n",
    "    campomaior_votacao = votacao_entities[votacao_entities['municipality'] == 'Campomaior']\n",
    "    campomaior_contabilizacao_entity_ids = set()\n",
    "    \n",
    "    # Get entity IDs that have contabilizacao relations\n",
    "    campomaior_relations = votations_relations[votations_relations['municipality'] == 'Campomaior']\n",
    "    if not campomaior_relations.empty:\n",
    "        # The 'resultado' relations connect entities, get both dependent and governor IDs\n",
    "        dependent_ids = set(campomaior_relations['dependent_id'].dropna().astype(int))\n",
    "        governor_ids = set(campomaior_relations['governor_id'].dropna().astype(int))\n",
    "        campomaior_contabilizacao_entity_ids = dependent_ids.union(governor_ids)\n",
    "    \n",
    "    # Debug information\n",
    "    print(f\"   Debug info:\")\n",
    "    print(f\"   - Total Campomaior VOTACAO entities: {len(campomaior_votacao)}\")\n",
    "    print(f\"   - Total Campomaior contabilizacao relations: {len(campomaior_relations)}\")\n",
    "    print(f\"   - Entity IDs involved in relations: {len(campomaior_contabilizacao_entity_ids)}\")\n",
    "    print(f\"   - Sample relation entity IDs: {list(campomaior_contabilizacao_entity_ids)[:10] if campomaior_contabilizacao_entity_ids else 'None'}\")\n",
    "    print(f\"   - Sample VOTACAO entity IDs: {list(campomaior_votacao['entity_id'].head(10))}\")\n",
    "    \n",
    "    # Find VOTACAO entities without contabilizacao\n",
    "    campomaior_without_contabilizacao = campomaior_votacao[\n",
    "        ~campomaior_votacao['entity_id'].isin(campomaior_contabilizacao_entity_ids)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n   \ud83d\udd0d Analysis Results:\")\n",
    "    print(f\"   Campomaior VOTACAO entities without contabilizacao: {len(campomaior_without_contabilizacao)}\")\n",
    "    if len(campomaior_without_contabilizacao) > 0:\n",
    "        print(\"   Examples (all):\")\n",
    "        for i, (_, entity) in enumerate(campomaior_without_contabilizacao.iterrows()):\n",
    "            print(f\"     {i+1}. ID: {entity['entity_id']}, Text: '{entity['text']}', File: {entity['filename']}\")\n",
    "    else:\n",
    "        print(\"   No VOTACAO entities found without contabilizacao relations\")\n",
    "    \n",
    "    # Also show examples of VOTACAO entities WITH contabilizacao for comparison\n",
    "    campomaior_with_contabilizacao = campomaior_votacao[\n",
    "        campomaior_votacao['entity_id'].isin(campomaior_contabilizacao_entity_ids)\n",
    "    ]\n",
    "    print(f\"   Campomaior VOTACAO entities WITH contabilizacao: {len(campomaior_with_contabilizacao)}\")\n",
    "    if len(campomaior_with_contabilizacao) > 0:\n",
    "        print(\"   Examples (first 5):\")\n",
    "        for i, (_, entity) in enumerate(campomaior_with_contabilizacao.head(5).iterrows()):\n",
    "            print(f\"     {i+1}. ID: {entity['entity_id']}, Text: '{entity['text']}', File: {entity['filename']}\")\n",
    "    \n",
    "    # Verify calculation consistency \n",
    "    calculated_gap = len(campomaior_votacao) - len(campomaior_relations)\n",
    "    actual_gap = len(campomaior_without_contabilizacao)\n",
    "    print(f\"\\n   \ud83d\udcca Gap Calculation Verification:\")\n",
    "    print(f\"   - Simple calculation (VOTACAO - relations): {calculated_gap}\")\n",
    "    print(f\"   - Entity-based calculation (without matching IDs): {actual_gap}\")\n",
    "    if calculated_gap != actual_gap:\n",
    "        print(f\"   \u26a0\ufe0f MISMATCH DETECTED! Investigating...\")\n",
    "        print(f\"   - This suggests some relations might involve non-VOTACAO entities\")\n",
    "    \n",
    "    # Create visualization - Main votation results\n",
    "    fig_votations = px.bar(\n",
    "        votations_by_muni.reset_index().melt(id_vars='municipality', var_name='result_type', value_name='count'),\n",
    "        x='municipality',\n",
    "        y='count',\n",
    "        color='result_type',\n",
    "        title='Votation Results by Municipality',\n",
    "        labels={'count': 'Number of Votations', 'municipality': 'Municipality', 'result_type': 'Result Type'},\n",
    "        color_discrete_sequence=px.colors.qualitative.Set3\n",
    "    )\n",
    "    \n",
    "    fig_votations.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500\n",
    "    )\n",
    "    fig_votations.show()\n",
    "    \n",
    "    # Additional analysis: Unanimity vs Majority focus\n",
    "    unanimity_majority = votations_relations[votations_relations['result_category'].isin(['Unanimity', 'Majority'])]\n",
    "    if not unanimity_majority.empty:\n",
    "        print(f\"\\n\ud83c\udfaf Unanimity vs Majority Analysis:\")\n",
    "        print(f\"   Total decisions with clear voting method: {len(unanimity_majority)}\")\n",
    "        \n",
    "        method_counts = unanimity_majority['result_category'].value_counts()\n",
    "        for method, count in method_counts.items():\n",
    "            pct = (count / len(unanimity_majority)) * 100\n",
    "            print(f\"   {method}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Note: Detailed breakdown by municipality is shown in the main votation results plot above\n",
    "    \n",
    "    # Coverage comparison visualization - showing the gap between VOTACAO entities and contabilizacao relations\n",
    "    coverage_data = []\n",
    "    for municipality in entities_df['municipality'].unique():\n",
    "        # Get VOTACAO entities for this municipality\n",
    "        muni_votacao_entities = votacao_entities[votacao_entities['municipality'] == municipality]\n",
    "        muni_total_votacao = len(muni_votacao_entities)\n",
    "        \n",
    "        # Get contabilizacao relations for this municipality\n",
    "        if municipality in votations_by_muni.index:\n",
    "            muni_contabilizacao_relations = votations_by_muni.loc[municipality].sum()\n",
    "        else:\n",
    "            muni_contabilizacao_relations = 0\n",
    "        \n",
    "        # Calculate actual VOTACAO entities involved in relations (CORRECTED LOGIC)\n",
    "        muni_relations = votations_relations[votations_relations['municipality'] == municipality]\n",
    "        muni_contabilizacao_entity_ids = set()\n",
    "        if not muni_relations.empty:\n",
    "            dependent_ids = set(muni_relations['dependent_id'].dropna().astype(int))\n",
    "            governor_ids = set(muni_relations['governor_id'].dropna().astype(int))\n",
    "            muni_contabilizacao_entity_ids = dependent_ids.union(governor_ids)\n",
    "        \n",
    "        # Count VOTACAO entities that are actually involved in relations\n",
    "        muni_votacao_with_relations = muni_votacao_entities[\n",
    "            muni_votacao_entities['entity_id'].isin(muni_contabilizacao_entity_ids)\n",
    "        ]\n",
    "        actual_coverage_count = len(muni_votacao_with_relations)\n",
    "        \n",
    "        # Calculate gap (VOTACAO entities without contabilizacao)\n",
    "        gap = muni_total_votacao - actual_coverage_count\n",
    "        coverage_pct = (actual_coverage_count / muni_total_votacao * 100) if muni_total_votacao > 0 else 0\n",
    "        \n",
    "        coverage_data.append({\n",
    "            'municipality': municipality,\n",
    "            'votacao_entities': muni_total_votacao,\n",
    "            'contabilizacao_relations': muni_contabilizacao_relations,\n",
    "            'votacao_with_contabilizacao': actual_coverage_count,\n",
    "            'votacao_without_contabilizacao': gap,\n",
    "            'coverage_percentage': coverage_pct\n",
    "        })\n",
    "    \n",
    "    coverage_df = pd.DataFrame(coverage_data)\n",
    "    # Filter out municipalities with no VOTACAO entities\n",
    "    coverage_df = coverage_df[coverage_df['votacao_entities'] > 0]\n",
    "    \n",
    "    fig_coverage = px.bar(\n",
    "        coverage_df,\n",
    "        x='municipality',\n",
    "        y=['votacao_entities', 'votacao_with_contabilizacao', 'votacao_without_contabilizacao'],\n",
    "        title='VOTACAO Entities vs Contabilizacao Relations Coverage Gap by Municipality',\n",
    "        labels={'value': 'Count', 'municipality': 'Municipality', 'variable': 'Data Type'},\n",
    "        color_discrete_map={\n",
    "            'votacao_entities': '#1f77b4',           # Blue - Total VOTACAO entities\n",
    "            'votacao_with_contabilizacao': '#2ca02c',   # Green - VOTACAO entities with contabilizacao\n",
    "            'votacao_without_contabilizacao': '#ff7f0e'  # Orange - Gap (VOTACAO entities without contabilizacao)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig_coverage.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500,\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig_coverage.show()\n",
    "    \n",
    "    # Save figures\n",
    "    if 'FIGURES_DIR' in globals():\n",
    "        fig_votations.write_html(FIGURES_DIR / 'votations_analysis.html')\n",
    "        fig_coverage.write_html(FIGURES_DIR / 'votations_coverage_analysis.html')\n",
    "        print(\"\ud83d\udcbe Votations analysis saved\")\n",
    "        \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No votation result data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92f240",
   "metadata": {},
   "source": [
    "- A diferen\u00e7a no campo maior tem a ver com aquelas deliberacoes multiplas no mesmo segmento. Quando dizem \"Mais foi deliberado\" nao h\u00e1 informa\u00e7\u00e3o explicita de contabilizacao, assume-se que \u00e9 por unanimidade pelo que vem de tr\u00e1s. \n",
    "- H\u00e1 alguns casos em que h\u00e1 mais do que uma contabilizacao por votacao."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ce942",
   "metadata": {},
   "source": [
    "## 6. Advanced Text Analysis with NLTK\n",
    "\n",
    "This section leverages the Natural Language Toolkit (NLTK) to perform advanced linguistic analysis on the Portuguese municipal documents. NLTK provides powerful tools for tokenization, morphological analysis, and text mining specifically designed for natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca786563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Setup and Portuguese Language Resources\n",
    "print(\"=== NLTK SETUP FOR PORTUGUESE TEXT ANALYSIS ===\")\n",
    "\n",
    "# Install and import NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"\u2705 NLTK already installed\")\n",
    "except ImportError:\n",
    "    print(\"\ud83d\udce6 Installing NLTK...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    print(\"\u2705 NLTK installed successfully\")\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"\\n\ud83d\udce5 Downloading NLTK resources...\")\n",
    "try:\n",
    "    # Portuguese stopwords\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"\u2705 Portuguese stopwords downloaded\")\n",
    "    \n",
    "    # Tokenizers\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"\u2705 Punkt tokenizer downloaded\")\n",
    "    \n",
    "    # Word tokenizer \n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"\u2705 Punkt tab tokenizer downloaded\")\n",
    "    \n",
    "    # POS tagger (if available)\n",
    "    try:\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        print(\"\u2705 POS tagger downloaded\")\n",
    "    except:\n",
    "        print(\"\u26a0\ufe0f POS tagger not available for Portuguese\")\n",
    "        \n",
    "    # N-gram and collocation tools\n",
    "    nltk.download('floresta', quiet=True)  # Portuguese treebank\n",
    "    print(\"\u2705 Floresta Portuguese corpus downloaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Some NLTK resources may not be available: {e}\")\n",
    "\n",
    "# Import necessary NLTK modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams, everygrams\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk import FreqDist, ConditionalFreqDist\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Set up Portuguese language resources\n",
    "try:\n",
    "    portuguese_stopwords = set(stopwords.words('portuguese'))\n",
    "    print(f\"\u2705 Loaded {len(portuguese_stopwords)} Portuguese stopwords\")\n",
    "except:\n",
    "    # Fallback Portuguese stopwords if NLTK ones aren't available\n",
    "    portuguese_stopwords = {\n",
    "        'a', 'ao', 'aos', 'aquela', 'aquele', 'aqueles', 'aquilo', 'as', 'at\u00e9', 'com', 'como', 'da', 'das', 'de', 'dela', 'dele', 'deles', 'do', 'dos', 'e', 'ela', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'esse', 'esta', 'estas', 'este', 'estes', 'eu', 'foi', 'for', 'foram', 'h\u00e1', 'isso', 'isto', 'j\u00e1', 'mas', 'mais', 'me', 'na', 'nas', 'no', 'nos', 'n\u00f3s', 'o', 'os', 'ou', 'para', 'pela', 'pelo', 'pelos', 'que', 'se', 'sem', 'ser', 'seu', 'seus', 'sua', 'suas', 'tamb\u00e9m', 'te', 'tem', 'tu', 'um', 'uma', 'voc\u00ea', 'voc\u00eas', '\u00e0', '\u00e9'\n",
    "    }\n",
    "    print(f\"\u2705 Using fallback Portuguese stopwords ({len(portuguese_stopwords)} words)\")\n",
    "\n",
    "print(\"\\n\ud83c\udfaf NLTK setup complete and ready for Portuguese text analysis!\")\n",
    "print(\"Available analysis capabilities:\")\n",
    "print(\"   \u2022 Tokenization (words and sentences)\")\n",
    "print(\"   \u2022 Stopword removal\") \n",
    "print(\"   \u2022 N-gram extraction (2-grams, 3-grams, etc.)\")\n",
    "print(\"   \u2022 Collocation finding\")\n",
    "print(\"   \u2022 Frequency analysis\")\n",
    "print(\"   \u2022 Text preprocessing and cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ASSUNTO entities with NLTK - COMPREHENSIVE VERSION\n",
    "print(\"\\n\ud83d\udd0d Analyzing ASSUNTO entities with comprehensive NLP techniques...\")\n",
    "\n",
    "# Filter ONLY for entities that have \"Tema\" field (actual topics)\n",
    "print(\"\ud83d\udd27 Filtering for ASSUNTO entities with 'Tema' field (actual topics only)...\")\n",
    "\n",
    "# Check available columns\n",
    "print(\"Available columns in entities_df:\")\n",
    "print(sorted(entities_df.columns))\n",
    "\n",
    "# Filter for ASSUNTO entities that have the \"Tema\" field populated\n",
    "# This excludes Fronteira markers that have voting terms\n",
    "assunto_entities = entities_df[\n",
    "    (entities_df['entity_label'] == 'Assunto') & \n",
    "    (entities_df['tema'].notna()) & \n",
    "    (entities_df['tema'].str.strip() != '')\n",
    "].copy()\n",
    "\n",
    "print(f\"\u2705 ASSUNTO entities with populated 'tema' field: {len(assunto_entities):,}\")\n",
    "\n",
    "if len(assunto_entities) == 0:\n",
    "    print(\"\u274c No ASSUNTO entities with 'tema' field found!\")\n",
    "    print(\"\\nDebugging - showing all ASSUNTO entities:\")\n",
    "    debug_assunto = entities_df[entities_df['entity_label'] == 'Assunto']\n",
    "    print(f\"Total ASSUNTO entities: {len(debug_assunto)}\")\n",
    "    if len(debug_assunto) > 0:\n",
    "        print(\"\\nSample ASSUNTO entities and their fields:\")\n",
    "        for idx, row in debug_assunto.head(5).iterrows():\n",
    "            print(f\"  - Annotated text: '{row.get('text', 'N/A')}'\")\n",
    "            print(f\"    Human tema: '{row.get('tema', 'N/A')}'\")\n",
    "            print(f\"    Fronteira: '{row.get('fronteira', 'N/A')}'\")\n",
    "            print()\n",
    "else:\n",
    "    # Extract municipality information from filename\n",
    "    assunto_entities['municipality'] = assunto_entities['filename'].str.extract(r'([^_]+)_')[0]\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccb Comparing human 'tema' titles vs actual annotated text:\")\n",
    "    print(f\"\ud83d\udcca Municipalities found: {sorted(assunto_entities['municipality'].unique())}\")\n",
    "    \n",
    "    # Show examples of both\n",
    "    sample_assuntos = assunto_entities.sample(min(5, len(assunto_entities)), random_state=42)\n",
    "    for i, (idx, row) in enumerate(sample_assuntos.iterrows(), 1):\n",
    "        print(f\"   {i:2d}. Municipality: {row['municipality']}\")\n",
    "        print(f\"       Human tema: '{row['tema']}'\")\n",
    "        print(f\"       Actual text: '{row['text']}'\")\n",
    "        print(f\"       Length comparison: tema={len(row['tema'])} chars, text={len(row['text'])} chars\")\n",
    "        print()\n",
    "    \n",
    "    # === TEMA ANALYSIS ===\n",
    "    print(f\"\\n\ud83d\udcca Analyzing human-written 'tema' titles...\")\n",
    "    tema_texts = assunto_entities['tema'].dropna()\n",
    "    \n",
    "    # Combine all tema texts for analysis\n",
    "    combined_tema_text = ' '.join(tema_texts)\n",
    "    print(f\"\ud83d\udcca Total characters in tema texts: {len(combined_tema_text):,}\")\n",
    "    \n",
    "    # Tokenize the tema texts\n",
    "    print(\"\ud83d\udd24 Tokenizing Portuguese tema texts...\")\n",
    "    tema_tokens = word_tokenize(combined_tema_text, language='portuguese')\n",
    "    print(f\"\ud83d\udcca Total tema tokens: {len(tema_tokens):,}\")\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tema_words = [word.lower() for word in tema_tokens if word.isalpha()]\n",
    "    print(f\"\ud83d\udcca Alphabetic words in temas: {len(tema_words):,}\")\n",
    "    \n",
    "    # Remove Portuguese stopwords\n",
    "    portuguese_stopwords = set(stopwords.words('portuguese'))\n",
    "    # Add common municipal/administrative stopwords\n",
    "    administrative_stopwords = {\n",
    "        'deliberado', 'aprovado', 'reuni\u00e3o', 'sess\u00e3o', 'c\u00e2mara', 'municipal', 'munic\u00edpio',\n",
    "        'proposta', 'pedido', 'solicita\u00e7\u00e3o', 'processo', 'delibera\u00e7\u00e3o', 'voto', 'votos'\n",
    "    }\n",
    "    all_stopwords = portuguese_stopwords.union(administrative_stopwords)\n",
    "    \n",
    "    filtered_tema_words = [word for word in tema_words if word not in all_stopwords and len(word) > 2]\n",
    "    print(f\"\ud83d\udcca Filtered meaningful words in temas: {len(filtered_tema_words):,}\")\n",
    "    \n",
    "    # === TEXT ANALYSIS ===\n",
    "    print(f\"\\n\ud83d\udd0d Analyzing actual annotated text content...\")\n",
    "    actual_texts = assunto_entities['text'].dropna()\n",
    "    combined_actual_text = ' '.join(actual_texts)\n",
    "    \n",
    "    # Tokenize actual texts\n",
    "    actual_tokens = word_tokenize(combined_actual_text, language='portuguese')\n",
    "    actual_words = [word.lower() for word in actual_tokens if word.isalpha()]\n",
    "    filtered_actual_words = [word for word in actual_words if word not in all_stopwords and len(word) > 2]\n",
    "    \n",
    "    print(f\"\ud83d\udcca Total characters in actual texts: {len(combined_actual_text):,}\")\n",
    "    print(f\"\ud83d\udcca Total actual tokens: {len(actual_tokens):,}\")\n",
    "    print(f\"\ud83d\udcca Filtered meaningful words in actual texts: {len(filtered_actual_words):,}\")\n",
    "    \n",
    "    # === FREQUENCY ANALYSIS ===\n",
    "    if len(filtered_tema_words) > 0:\n",
    "        # Frequency analysis of tema titles\n",
    "        print(\"\\n\ud83d\udcca Most frequent terms in human 'tema' titles:\")\n",
    "        tema_freq_dist = FreqDist(filtered_tema_words)\n",
    "        most_common_tema = tema_freq_dist.most_common(20)\n",
    "        \n",
    "        for i, (word, freq) in enumerate(most_common_tema, 1):\n",
    "            print(f\"   {i:2d}. '{word}': {freq:,} occurrences\")\n",
    "            \n",
    "        # Frequency analysis of actual texts\n",
    "        print(\"\\n\ud83d\udcca Most frequent terms in actual annotated texts:\")\n",
    "        actual_freq_dist = FreqDist(filtered_actual_words)\n",
    "        most_common_actual = actual_freq_dist.most_common(20)\n",
    "        \n",
    "        for i, (word, freq) in enumerate(most_common_actual, 1):\n",
    "            print(f\"   {i:2d}. '{word}': {freq:,} occurrences\")\n",
    "    \n",
    "    # === N-GRAM ANALYSIS ===\n",
    "    print(\"\\n\ud83d\udd17 Extracting bigrams and trigrams...\")\n",
    "    \n",
    "    # Tema bigrams\n",
    "    tema_bigrams = list(ngrams(filtered_tema_words, 2))\n",
    "    tema_bigram_freq = FreqDist(tema_bigrams)\n",
    "    most_common_tema_bigrams = tema_bigram_freq.most_common(10)\n",
    "    \n",
    "    # Actual text bigrams\n",
    "    actual_bigrams = list(ngrams(filtered_actual_words, 2))\n",
    "    actual_bigram_freq = FreqDist(actual_bigrams)\n",
    "    most_common_actual_bigrams = actual_bigram_freq.most_common(10)\n",
    "    \n",
    "    print(\"\ud83d\udcca Most common tema bigrams:\")\n",
    "    for i, (bigram, freq) in enumerate(most_common_tema_bigrams, 1):\n",
    "        print(f\"   {i:2d}. '{' '.join(bigram)}': {freq} occurrences\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Most common actual text bigrams:\")\n",
    "    for i, (bigram, freq) in enumerate(most_common_actual_bigrams, 1):\n",
    "        print(f\"   {i:2d}. '{' '.join(bigram)}': {freq} occurrences\")\n",
    "    \n",
    "    # === MUNICIPALITY ANALYSIS ===\n",
    "    print(\"\\n\ud83c\udfdb\ufe0f Municipality-wise analysis...\")\n",
    "    municipality_stats = {}\n",
    "    \n",
    "    for municipality in assunto_entities['municipality'].unique():\n",
    "        muni_data = assunto_entities[assunto_entities['municipality'] == municipality]\n",
    "        \n",
    "        # Combine texts for this municipality\n",
    "        muni_tema_text = ' '.join(muni_data['tema'].dropna())\n",
    "        muni_actual_text = ' '.join(muni_data['text'].dropna())\n",
    "        \n",
    "        # Tokenize and process\n",
    "        muni_tema_tokens = word_tokenize(muni_tema_text, language='portuguese')\n",
    "        muni_tema_words = [w.lower() for w in muni_tema_tokens if w.isalpha() and w.lower() not in all_stopwords and len(w) > 2]\n",
    "        \n",
    "        muni_actual_tokens = word_tokenize(muni_actual_text, language='portuguese')\n",
    "        muni_actual_words = [w.lower() for w in muni_actual_tokens if w.isalpha() and w.lower() not in all_stopwords and len(w) > 2]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        municipality_stats[municipality] = {\n",
    "            'total_entities': len(muni_data),\n",
    "            'tema_tokens': len(muni_tema_words),\n",
    "            'actual_tokens': len(muni_actual_words),\n",
    "            'tema_unique': len(set(muni_tema_words)),\n",
    "            'actual_unique': len(set(muni_actual_words)),\n",
    "            'tema_lexical_diversity': len(set(muni_tema_words)) / max(1, len(muni_tema_words)),\n",
    "            'actual_lexical_diversity': len(set(muni_actual_words)) / max(1, len(muni_actual_words)),\n",
    "            'avg_tema_length': muni_data['tema'].str.len().mean(),\n",
    "            'avg_text_length': muni_data['text'].str.len().mean(),\n",
    "            'most_common_tema_words': FreqDist(muni_tema_words).most_common(5),\n",
    "            'most_common_actual_words': FreqDist(muni_actual_words).most_common(5)\n",
    "        }\n",
    "    \n",
    "    print(\"\ud83d\udcca Municipality statistics:\")\n",
    "    for municipality, stats in municipality_stats.items():\n",
    "        print(f\"\\n   \ud83c\udfdb\ufe0f {municipality}:\")\n",
    "        print(f\"      \u2022 Total entities: {stats['total_entities']}\")\n",
    "        print(f\"      \u2022 Tema lexical diversity: {stats['tema_lexical_diversity']:.3f}\")\n",
    "        print(f\"      \u2022 Actual text lexical diversity: {stats['actual_lexical_diversity']:.3f}\")\n",
    "        print(f\"      \u2022 Avg tema length: {stats['avg_tema_length']:.1f} chars\")\n",
    "        print(f\"      \u2022 Avg text length: {stats['avg_text_length']:.1f} chars\")\n",
    "    \n",
    "    # === LENGTH COMPARISON ANALYSIS ===\n",
    "    print(\"\\n\ud83d\udccf Length comparison analysis...\")\n",
    "    assunto_entities['tema_char_length'] = assunto_entities['tema'].str.len()\n",
    "    assunto_entities['text_char_length'] = assunto_entities['text'].str.len()\n",
    "    assunto_entities['length_ratio'] = assunto_entities['tema_char_length'] / assunto_entities['text_char_length']\n",
    "    \n",
    "    print(f\"\ud83d\udcca Average tema length: {assunto_entities['tema_char_length'].mean():.1f} characters\")\n",
    "    print(f\"\ud83d\udcca Average text length: {assunto_entities['text_char_length'].mean():.1f} characters\")\n",
    "    print(f\"\ud83d\udcca Average length ratio (tema/text): {assunto_entities['length_ratio'].mean():.2f}\")\n",
    "    print(f\"\ud83d\udcca Tema tends to be {'longer' if assunto_entities['length_ratio'].mean() > 1 else 'shorter'} than actual text\")\n",
    "    \n",
    "    # Store variables for visualization in next cell\n",
    "    print(f\"\\n\u2705 Comprehensive NLTK analysis complete!\")\n",
    "    print(f\"\ud83d\udcca Variables ready for visualization:\")\n",
    "    print(f\"   \u2022 tema_freq_dist, actual_freq_dist: Word frequency distributions\")\n",
    "    print(f\"   \u2022 tema_bigram_freq, actual_bigram_freq: Bigram frequency distributions\") \n",
    "    print(f\"   \u2022 municipality_stats: Per-municipality statistics\")\n",
    "    print(f\"   \u2022 assunto_entities: Enhanced dataframe with length metrics\")\n",
    "    print(f\"   \u2022 Length analysis: Character counts and ratios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db107cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean surrogate characters from data to fix UnicodeEncodeError\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "\n",
    "def clean_surrogates(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove surrogate characters that cause encoding issues\n",
    "        return re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to relevant data structures\n",
    "if 'assunto_entities' in globals():\n",
    "    assunto_entities = assunto_entities.apply(lambda col: col.map(clean_surrogates) if col.dtype == 'object' else col)\n",
    "\n",
    "if 'tema_freq_dist' in globals():\n",
    "    cleaned_tema = {clean_surrogates(k): v for k, v in tema_freq_dist.items()}\n",
    "    tema_freq_dist = FreqDist(cleaned_tema)\n",
    "\n",
    "if 'actual_freq_dist' in globals():\n",
    "    cleaned_actual = {clean_surrogates(k): v for k, v in actual_freq_dist.items()}\n",
    "    actual_freq_dist = FreqDist(cleaned_actual)\n",
    "\n",
    "if 'tema_bigram_freq' in globals():\n",
    "    cleaned_tema_bigram = {(clean_surrogates(b[0]), clean_surrogates(b[1])): v for b, v in tema_bigram_freq.items()}\n",
    "    tema_bigram_freq = FreqDist(cleaned_tema_bigram)\n",
    "\n",
    "if 'actual_bigram_freq' in globals():\n",
    "    cleaned_actual_bigram = {(clean_surrogates(b[0]), clean_surrogates(b[1])): v for b, v in actual_bigram_freq.items()}\n",
    "    actual_bigram_freq = FreqDist(cleaned_actual_bigram)\n",
    "\n",
    "if 'municipality_stats' in globals():\n",
    "    for m in municipality_stats:\n",
    "        for key in ['most_common_tema_words', 'most_common_actual_words']:\n",
    "            if key in municipality_stats[m]:\n",
    "                municipality_stats[m][key] = [(clean_surrogates(word), freq) for word, freq in municipality_stats[m][key]]\n",
    "\n",
    "print(\"\u2705 Surrogate characters cleaned from data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE VISUALIZATIONS FOR NLTK ANALYSIS\n",
    "print(\"=== CREATING COMPREHENSIVE NLTK VISUALIZATIONS ===\")\n",
    "\n",
    "# Install wordcloud if not available\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    print(\"\u2705 WordCloud library available\")\n",
    "except ImportError:\n",
    "    print(\"\ud83d\udce6 Installing wordcloud library...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wordcloud\"])\n",
    "    from wordcloud import WordCloud\n",
    "    print(\"\u2705 WordCloud library installed\")\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Check if we have the required data from cell 27\n",
    "if 'assunto_entities' not in globals() or len(assunto_entities) == 0:\n",
    "    print(\"\u274c No ASSUNTO entities data available. Please run cell 27 first.\")\n",
    "else:\n",
    "    print(f\"\u2705 Processing {len(assunto_entities):,} ASSUNTO entities for visualization\")\n",
    "    \n",
    "    # === 1. WORD CLOUDS ===\n",
    "    print(\"\\n Creating word clouds...\")\n",
    "    \n",
    "    # Create word clouds for tema and actual text\n",
    "    fig_wordclouds = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=['Tema Words (Human-written)', 'Actual Text Words (Machine-extracted)',\n",
    "                       'Tema Bigrams', 'Actual Text Bigrams'],\n",
    "        specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "               [{\"type\": \"xy\"}, {\"type\": \"xy\"}]]\n",
    "    )\n",
    "    \n",
    "    # Word Cloud 1: Tema words\n",
    "    if 'tema_freq_dist' in globals():\n",
    "        wordcloud_tema = WordCloud(\n",
    "            width=800, height=400, \n",
    "            background_color='white',\n",
    "            max_words=100,\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(dict(tema_freq_dist))\n",
    "        \n",
    "        # Save wordcloud as image temporarily for display\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(wordcloud_tema, interpolation='bilinear')\n",
    "        plt.title('Tema Words (Human-written)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Word Cloud 2: Actual text words\n",
    "        if 'actual_freq_dist' in globals():\n",
    "            wordcloud_actual = WordCloud(\n",
    "                width=800, height=400,\n",
    "                background_color='white', \n",
    "                max_words=100,\n",
    "                colormap='plasma'\n",
    "            ).generate_from_frequencies(dict(actual_freq_dist))\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(wordcloud_actual, interpolation='bilinear')\n",
    "            plt.title('Actual Text Words (Machine-extracted)')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # === 2. FREQUENCY COMPARISON BAR CHARTS ===\n",
    "    print(\"\\n\ud83d\udcca Creating frequency comparison charts...\")\n",
    "    \n",
    "    if 'tema_freq_dist' in globals() and 'actual_freq_dist' in globals():\n",
    "        # Top words comparison\n",
    "        top_tema_words = dict(tema_freq_dist.most_common(15))\n",
    "        top_actual_words = dict(actual_freq_dist.most_common(15))\n",
    "        \n",
    "        fig_freq_comparison = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=['Most Frequent Words in Tema', 'Most Frequent Words in Actual Text'],\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        # Tema frequency chart\n",
    "        fig_freq_comparison.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(top_tema_words.values()),\n",
    "                y=list(top_tema_words.keys()),\n",
    "                orientation='h',\n",
    "                name='Tema Words',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Actual text frequency chart\n",
    "        fig_freq_comparison.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(top_actual_words.values()),\n",
    "                y=list(top_actual_words.keys()),\n",
    "                orientation='h',\n",
    "                name='Actual Text Words',\n",
    "                marker_color='lightcoral'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig_freq_comparison.update_layout(\n",
    "            height=600,\n",
    "            title_text=\"Word Frequency Comparison: Tema vs Actual Text\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig_freq_comparison.show()\n",
    "    \n",
    "    # === 3. BIGRAM ANALYSIS ===\n",
    "    print(\"\\n\ud83d\udd17 Creating bigram analysis charts...\")\n",
    "    \n",
    "    if 'tema_bigram_freq' in globals() and 'actual_bigram_freq' in globals():\n",
    "        # Bigram comparison\n",
    "        top_tema_bigrams = [(f\"{b[0]} {b[1]}\", f) for b, f in tema_bigram_freq.most_common(10)]\n",
    "        top_actual_bigrams = [(f\"{b[0]} {b[1]}\", f) for b, f in actual_bigram_freq.most_common(10)]\n",
    "        \n",
    "        fig_bigrams = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=['Most Common Bigrams in Tema', 'Most Common Bigrams in Actual Text']\n",
    "        )\n",
    "        \n",
    "        # Tema bigrams\n",
    "        if top_tema_bigrams:\n",
    "            fig_bigrams.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[freq for _, freq in top_tema_bigrams],\n",
    "                    y=[bigram for bigram, _ in top_tema_bigrams],\n",
    "                    orientation='h',\n",
    "                    name='Tema Bigrams',\n",
    "                    marker_color='skyblue'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Actual text bigrams\n",
    "        if top_actual_bigrams:\n",
    "            fig_bigrams.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[freq for _, freq in top_actual_bigrams],\n",
    "                    y=[bigram for bigram, _ in top_actual_bigrams],\n",
    "                    orientation='h',\n",
    "                    name='Actual Text Bigrams',\n",
    "                    marker_color='salmon'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        fig_bigrams.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Bigram Frequency Analysis\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig_bigrams.show()\n",
    "    \n",
    "    # === 4. MUNICIPALITY COMPARISON ===\n",
    "    print(\"\\n\ud83c\udfdb\ufe0f Creating municipality comparison charts...\")\n",
    "    \n",
    "    if 'municipality_stats' in globals():\n",
    "        municipalities = list(municipality_stats.keys())\n",
    "        \n",
    "        # Extract statistics for plotting\n",
    "        total_entities = [municipality_stats[m]['total_entities'] for m in municipalities]\n",
    "        tema_diversity = [municipality_stats[m]['tema_lexical_diversity'] for m in municipalities]\n",
    "        actual_diversity = [municipality_stats[m]['actual_lexical_diversity'] for m in municipalities]\n",
    "        avg_tema_length = [municipality_stats[m]['avg_tema_length'] for m in municipalities]\n",
    "        avg_text_length = [municipality_stats[m]['avg_text_length'] for m in municipalities]\n",
    "        \n",
    "        # Municipality comparison dashboard\n",
    "        fig_muni = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'Total ASSUNTO Entities by Municipality',\n",
    "                'Lexical Diversity Comparison',\n",
    "                'Average Text Length by Municipality',\n",
    "                'Length Comparison: Tema vs Actual Text'\n",
    "            ],\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "        \n",
    "        # Total entities\n",
    "        fig_muni.add_trace(\n",
    "            go.Bar(x=municipalities, y=total_entities, name='Total Entities',\n",
    "                   marker_color='lightgreen'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Lexical diversity scatter plot\n",
    "        fig_muni.add_trace(\n",
    "            go.Scatter(\n",
    "                x=tema_diversity, y=actual_diversity,\n",
    "                mode='markers+text',\n",
    "                text=municipalities,\n",
    "                textposition='top center',\n",
    "                marker=dict(size=12, color='darkblue'),\n",
    "                name='Lexical Diversity'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Average lengths\n",
    "        fig_muni.add_trace(\n",
    "            go.Bar(x=municipalities, y=avg_tema_length, name='Avg Tema Length',\n",
    "                   marker_color='lightblue'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Length comparison scatter\n",
    "        fig_muni.add_trace(\n",
    "            go.Scatter(\n",
    "                x=avg_tema_length, y=avg_text_length,\n",
    "                mode='markers+text',\n",
    "                text=municipalities,\n",
    "                textposition='top center',\n",
    "                marker=dict(size=12, color='darkred'),\n",
    "                name='Length Comparison'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Add diagonal line for length comparison\n",
    "        max_length = max(max(avg_tema_length), max(avg_text_length))\n",
    "        fig_muni.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, max_length], y=[0, max_length],\n",
    "                mode='lines',\n",
    "                line=dict(dash='dash', color='gray'),\n",
    "                name='Equal Length Line',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig_muni.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Municipality-wise ASSUNTO Analysis\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Update axis labels\n",
    "        fig_muni.update_xaxes(title_text=\"Municipality\", row=1, col=1)\n",
    "        fig_muni.update_xaxes(title_text=\"Tema Lexical Diversity\", row=1, col=2)\n",
    "        fig_muni.update_xaxes(title_text=\"Municipality\", row=2, col=1)\n",
    "        fig_muni.update_xaxes(title_text=\"Avg Tema Length\", row=2, col=2)\n",
    "        \n",
    "        fig_muni.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "        fig_muni.update_yaxes(title_text=\"Actual Text Lexical Diversity\", row=1, col=2)\n",
    "        fig_muni.update_yaxes(title_text=\"Characters\", row=2, col=1)\n",
    "        fig_muni.update_yaxes(title_text=\"Avg Actual Text Length\", row=2, col=2)\n",
    "        \n",
    "        fig_muni.show()\n",
    "    \n",
    "    # === 5. LENGTH DISTRIBUTION ANALYSIS ===\n",
    "    print(\"\\n\ud83d\udccf Creating length distribution analysis...\")\n",
    "    \n",
    "    if 'tema_char_length' in assunto_entities.columns:\n",
    "        # Length distributions\n",
    "        fig_length = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'Tema Character Length Distribution',\n",
    "                'Actual Text Character Length Distribution',\n",
    "                'Length Ratio Distribution (Tema/Text)',\n",
    "                'Length Correlation: Tema vs Actual Text'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Tema length histogram\n",
    "        fig_length.add_trace(\n",
    "            go.Histogram(\n",
    "                x=assunto_entities['tema_char_length'],\n",
    "                nbinsx=30,\n",
    "                name='Tema Length',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Actual text length histogram\n",
    "        fig_length.add_trace(\n",
    "            go.Histogram(\n",
    "                x=assunto_entities['text_char_length'],\n",
    "                nbinsx=30,\n",
    "                name='Text Length',\n",
    "                marker_color='lightcoral'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Length ratio histogram\n",
    "        fig_length.add_trace(\n",
    "            go.Histogram(\n",
    "                x=assunto_entities['length_ratio'],\n",
    "                nbinsx=30,\n",
    "                name='Length Ratio',\n",
    "                marker_color='lightgreen'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Correlation scatter plot\n",
    "        fig_length.add_trace(\n",
    "            go.Scatter(\n",
    "                x=assunto_entities['tema_char_length'],\n",
    "                y=assunto_entities['text_char_length'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=4, color='darkblue', opacity=0.6),\n",
    "                name='Length Correlation'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Add correlation line\n",
    "        correlation = np.corrcoef(assunto_entities['tema_char_length'], \n",
    "                                 assunto_entities['text_char_length'])[0, 1]\n",
    "        \n",
    "        # Add trendline\n",
    "        z = np.polyfit(assunto_entities['tema_char_length'], \n",
    "                      assunto_entities['text_char_length'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(assunto_entities['tema_char_length'].min(), \n",
    "                             assunto_entities['tema_char_length'].max(), 100)\n",
    "        \n",
    "        fig_length.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_trend,\n",
    "                y=p(x_trend),\n",
    "                mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name=f'Trendline (r={correlation:.3f})',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig_length.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Length Analysis: Tema vs Actual Text\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig_length.show()\n",
    "    \n",
    "    # === 6. SUMMARY STATISTICS ===\n",
    "    print(\"\\n\ud83d\udcca Summary Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'tema_freq_dist' in globals():\n",
    "        print(f\"\ud83d\udcca Vocabulary Statistics:\")\n",
    "        print(f\"   \u2022 Unique words in tema: {len(tema_freq_dist):,}\")\n",
    "        print(f\"   \u2022 Unique words in actual text: {len(actual_freq_dist):,}\")\n",
    "        print(f\"   \u2022 Overlap: {len(set(tema_freq_dist.keys()) & set(actual_freq_dist.keys())):,} words\")\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        tema_words_set = set(tema_freq_dist.keys())\n",
    "        actual_words_set = set(actual_freq_dist.keys())\n",
    "        jaccard_similarity = len(tema_words_set & actual_words_set) / len(tema_words_set | actual_words_set)\n",
    "        print(f\"   \u2022 Jaccard similarity: {jaccard_similarity:.3f}\")\n",
    "    \n",
    "    if 'municipality_stats' in globals():\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f Municipality Insights:\")\n",
    "        best_diversity_tema = max(municipality_stats.items(), \n",
    "                                 key=lambda x: x[1]['tema_lexical_diversity'])\n",
    "        best_diversity_actual = max(municipality_stats.items(), \n",
    "                                   key=lambda x: x[1]['actual_lexical_diversity'])\n",
    "        most_entities = max(municipality_stats.items(), \n",
    "                           key=lambda x: x[1]['total_entities'])\n",
    "        \n",
    "        print(f\"   \u2022 Highest tema lexical diversity: {best_diversity_tema[0]} ({best_diversity_tema[1]['tema_lexical_diversity']:.3f})\")\n",
    "        print(f\"   \u2022 Highest actual text lexical diversity: {best_diversity_actual[0]} ({best_diversity_actual[1]['actual_lexical_diversity']:.3f})\")\n",
    "        print(f\"   \u2022 Most entities: {most_entities[0]} ({most_entities[1]['total_entities']} entities)\")\n",
    "    \n",
    "    if 'tema_char_length' in assunto_entities.columns:\n",
    "        print(f\"\\n\ud83d\udcca Length Analysis:\")\n",
    "        print(f\"   \u2022 Average tema length: {assunto_entities['tema_char_length'].mean():.1f} \u00b1 {assunto_entities['tema_char_length'].std():.1f} chars\")\n",
    "        print(f\"   \u2022 Average text length: {assunto_entities['text_char_length'].mean():.1f} \u00b1 {assunto_entities['text_char_length'].std():.1f} chars\")\n",
    "        print(f\"   \u2022 Length correlation: {correlation:.3f}\")\n",
    "        print(f\"   \u2022 Median length ratio: {assunto_entities['length_ratio'].median():.2f}\")\n",
    "    \n",
    "    # Save figures if directories exist\n",
    "    if 'FIGURES_DIR' in globals():\n",
    "        try:\n",
    "            fig_freq_comparison.write_html(FIGURES_DIR / 'nltk_frequency_comparison.html')\n",
    "            fig_bigrams.write_html(FIGURES_DIR / 'nltk_bigram_analysis.html')\n",
    "            fig_muni.write_html(FIGURES_DIR / 'nltk_municipality_analysis.html')\n",
    "            fig_length.write_html(FIGURES_DIR / 'nltk_length_analysis.html')\n",
    "            print(\"\\n\ud83d\udcbe All visualizations saved to figures directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Could not save some figures: {e}\")\n",
    "    \n",
    "    print(\"\\n\u2705 Comprehensive NLTK visualization analysis complete!\")\n",
    "    print(\"\ud83c\udfaf Key insights revealed:\")\n",
    "    print(\"   \u2022 Word usage patterns in human vs machine text\")\n",
    "    print(\"   \u2022 Municipality-specific linguistic characteristics\") \n",
    "    print(\"   \u2022 Length relationships between tema and actual text\")\n",
    "    print(\"   \u2022 Lexical diversity and vocabulary overlap analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b58881",
   "metadata": {},
   "source": [
    "- TODO: Adicionar aqui testes estat\u00edstico de forma semelhante ao que foi feito noutras secc\u00e7\u00f5es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y75tfiul9wk",
   "metadata": {},
   "source": [
    "## 8. Fronteiras Analysis (Boundaries/Limits)\n",
    "\n",
    "Analysis of **fronteiras** (boundaries or limits) annotations, which represent administrative, geographical, or conceptual boundaries mentioned in municipal documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jsppanw4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fronteiras analysis\n",
    "fronteiras_analysis = comprehensive_analysis.get('fronteiras_analysis', {})\n",
    "\n",
    "if 'error' not in fronteiras_analysis and fronteiras_analysis:\n",
    "    print(\"=== FRONTEIRAS ANALYSIS ===\")\n",
    "    print(f\"\ud83c\udfdb\ufe0f Total Fronteira entities: {fronteiras_analysis['total_fronteira_entities']:,}\")\n",
    "    print(f\"\ud83d\udcc4 Documents with fronteira: {fronteiras_analysis['documents_with_fronteira']:,}\")\n",
    "    \n",
    "    # Fronteira type distribution\n",
    "    type_dist = fronteiras_analysis.get('fronteira_type_distribution', {})\n",
    "    if type_dist:\n",
    "        print(f\"\\n\ud83d\udcca Fronteira Type Distribution:\")\n",
    "        for f_type, count in type_dist.items():\n",
    "            percentage = fronteiras_analysis.get('fronteira_type_percentages', {}).get(f_type, 0)\n",
    "            print(f\"   \u2022 {f_type}: {count:,} ({percentage}%)\")\n",
    "    \n",
    "    # Municipality distribution\n",
    "    muni_dist = fronteiras_analysis.get('fronteira_by_municipality', {})\n",
    "    if muni_dist:\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f Fronteiras by Municipality:\")\n",
    "        for fronteira_type, municipalities in muni_dist.items():\n",
    "            print(f\"   {fronteira_type}:\")\n",
    "            for muni, count in municipalities.items():\n",
    "                print(f\"      \u2022 {muni}: {count:,}\")\n",
    "    \n",
    "    # Entity co-occurrence\n",
    "    cooccurrence = fronteiras_analysis.get('fronteira_entity_cooccurrence', {})\n",
    "    if cooccurrence:\n",
    "        print(f\"\\n\ud83c\udff7\ufe0f Entity Co-occurrence with Fronteiras:\")\n",
    "        for entity_type, fronteira_types in cooccurrence.items():\n",
    "            print(f\"   {entity_type}:\")\n",
    "            for f_type, count in fronteira_types.items():\n",
    "                print(f\"      \u2022 {f_type}: {count:,}\")\n",
    "    \n",
    "    # Text statistics\n",
    "    text_stats = fronteiras_analysis.get('fronteira_text_statistics', {})\n",
    "    if text_stats:\n",
    "        print(f\"\\n\ud83d\udcdd Fronteira Text Statistics:\")\n",
    "        print(f\"   \u2022 Average length: {text_stats['avg_length_chars']:.1f} characters, {text_stats['avg_length_tokens']:.1f} tokens\")\n",
    "        \n",
    "        length_by_type = text_stats.get('length_by_fronteira_type', {})\n",
    "        if length_by_type and 'mean' in length_by_type:\n",
    "            print(f\"   \u2022 Length by type:\")\n",
    "            for f_type, avg_len in length_by_type['mean'].items():\n",
    "                std_len = length_by_type.get('std', {}).get(f_type, 0)\n",
    "                count = length_by_type.get('count', {}).get(f_type, 0)\n",
    "                print(f\"      - {f_type}: {avg_len:.1f}\u00b1{std_len:.1f} chars ({count} entities)\")\n",
    "\n",
    "else:\n",
    "    print(\"=== FRONTEIRAS ANALYSIS ===\")\n",
    "    if 'error' in fronteiras_analysis:\n",
    "        print(f\"\u26a0\ufe0f Error in fronteiras analysis: {fronteiras_analysis['error']}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No fronteiras data found or analysis empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44bcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the fronteiras analysis visualization (corrected)\n",
    "print(\"=== FRONTEIRAS ANALYSIS ===\")\n",
    "\n",
    "# Get fronteira data\n",
    "fronteiras_data = entities_df[entities_df['fronteira'].notna()]\n",
    "\n",
    "if not fronteiras_data.empty:\n",
    "    print(\"\ud83d\udd0d Fronteira Analysis:\")\n",
    "    \n",
    "    # Fronteira type distribution\n",
    "    fronteira_counts = fronteiras_data['fronteira'].value_counts()\n",
    "    total_fronteiras = len(fronteiras_data)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Fronteira Distribution ({total_fronteiras:,} total entities):\")\n",
    "    for fronteira_type, count in fronteira_counts.items():\n",
    "        pct = (count / total_fronteiras) * 100\n",
    "        print(f\"   {fronteira_type}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Create visualization with corrected specs and values\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    fig_fronteiras = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Fronteira Type Distribution', 'Fronteira by Municipality'),\n",
    "        specs=[[{'type': 'pie'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Pie chart for distribution\n",
    "    fig_fronteiras.add_trace(\n",
    "        go.Pie(\n",
    "            labels=list(fronteira_counts.keys()),\n",
    "            values=list(fronteira_counts.values),  # Fixed: .values is a property, not method\n",
    "            name=\"Fronteira Types\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Bar chart by municipality\n",
    "    fronteira_muni = fronteiras_data.groupby(['municipality', 'fronteira']).size().unstack(fill_value=0)\n",
    "    \n",
    "    for fronteira_type in fronteira_muni.columns:\n",
    "        fig_fronteiras.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(fronteira_muni.index),\n",
    "                y=list(fronteira_muni[fronteira_type]),\n",
    "                name=fronteira_type,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig_fronteiras.update_layout(height=500, title_text=\"Fronteira Analysis\")\n",
    "    fig_fronteiras.show()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc8 Municipalities with fronteira data: {fronteiras_data['municipality'].nunique()}\")\n",
    "    print(f\"\ud83d\udcc8 Documents with fronteira data: {fronteiras_data['filename'].nunique()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No fronteira data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40711253",
   "metadata": {},
   "source": [
    "- Eventualmente quando a an\u00e1lise da sumariza\u00e7\u00e3o for feita, ser\u00e1 relevante fazer uma compara\u00e7\u00e3o do texto entre fronteiras e do texto sumarizado. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oi0fbc4xvmh",
   "metadata": {},
   "source": [
    "## 10. Metadata Analysis (Meeting Information)\n",
    "\n",
    "Comprehensive analysis of document metadata including meeting types, participants, presence information, political parties, and scheduling patterns. \n",
    "\n",
    "Importante ter em conta que os partidos adv\u00e9m de uma base de dados externa e nao se encontram explicitamente referidos na ata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o20wr3aatfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata analysis\n",
    "metadata_analysis = comprehensive_analysis.get('metadata_analysis', {})\n",
    "\n",
    "if 'error' not in metadata_analysis and metadata_analysis:\n",
    "    print(\"=== METADATA ANALYSIS ===\")\n",
    "\n",
    "    # Meeting type analysis\n",
    "    if 'meeting_type_analysis' in metadata_analysis:\n",
    "        meeting_types = metadata_analysis['meeting_type_analysis']\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f Meeting Type Analysis:\")\n",
    "        print(f\"   Total with meeting type info: {meeting_types.get('total_with_meeting_type', 0):,}\")\n",
    "\n",
    "        types_dist = meeting_types.get('meeting_types', {})\n",
    "        types_pct = meeting_types.get('meeting_type_percentages', {})\n",
    "\n",
    "        for meeting_type, count in types_dist.items():\n",
    "            pct = types_pct.get(meeting_type, 0)\n",
    "            print(f\"     {meeting_type}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # Political party analysis\n",
    "    if 'political_party_analysis' in metadata_analysis:\n",
    "        party_analysis = metadata_analysis['political_party_analysis']\n",
    "        print(f\"\\n\ud83c\udfdb\ufe0f Political Party Analysis:\")\n",
    "        print(f\"   Total with party info: {party_analysis.get('total_with_party_info', 0):,}\")\n",
    "        print(f\"   Unique parties: {party_analysis.get('unique_parties', 0)}\")\n",
    "\n",
    "        party_dist = party_analysis.get('party_distribution', {})\n",
    "        party_pct = party_analysis.get('party_percentages', {})\n",
    "\n",
    "        print(f\"\\n   Top Political Parties:\")\n",
    "        for party, count in sorted(party_dist.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            pct = party_pct.get(party, 0)\n",
    "            print(f\"     {party}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "        # === LEFT VS RIGHT PARTY ANALYSIS ===\n",
    "        print(f\"\\n\u2696\ufe0f Left vs Right Political Spectrum Analysis:\")\n",
    "\n",
    "        # Define Portuguese political parties by spectrum with precise matching\n",
    "        left_parties = ['PS', 'PCP', 'BE', 'PEV', 'LIVRE']\n",
    "        right_parties = ['PSD', 'CDS-PP', 'CDS', 'CHEGA', 'IL', 'PPM', 'PPD/PSD', 'PPD/PSD.CDS-PP', 'CDS-PP/PSD', 'RM', \"N\u00f3s, Cidad\u00e3os!\"]\n",
    "        center_parties = ['PAN']\n",
    "\n",
    "        # Categorize parties with improved logic\n",
    "        left_count = 0\n",
    "        right_count = 0\n",
    "        center_count = 0\n",
    "        other_count = 0\n",
    "\n",
    "        left_party_details = {}\n",
    "        right_party_details = {}\n",
    "        center_party_details = {}\n",
    "        other_party_details = {}\n",
    "\n",
    "        def classify_party(party_name):\n",
    "            \"\"\"Classify a party using precise matching logic\"\"\"\n",
    "            party_upper = party_name.upper()\n",
    "\n",
    "            # Check for exact matches first\n",
    "            if party_name in left_parties:\n",
    "                return 'left'\n",
    "            if party_name in right_parties:\n",
    "                return 'right'\n",
    "            if party_name in center_parties:\n",
    "                return 'center'\n",
    "\n",
    "            # Check for coalition parties\n",
    "            if 'PSD' in party_upper and 'PS' not in party_upper.split('/')[0]:  # PSD but not starting with PS\n",
    "                return 'right'\n",
    "            if 'CDS' in party_upper:\n",
    "                return 'right'\n",
    "            if 'PPD' in party_upper:\n",
    "                return 'right'\n",
    "\n",
    "            # Check for left parties (avoid false positives)\n",
    "            if party_upper == 'PS' or party_upper.startswith('PS/') or '/PS' in party_upper:\n",
    "                return 'left'\n",
    "            if 'PCP' in party_upper:\n",
    "                return 'left'\n",
    "            if 'BE' in party_upper:\n",
    "                return 'left'\n",
    "            if 'PEV' in party_upper:\n",
    "                return 'left'\n",
    "            if 'LIVRE' in party_upper:\n",
    "                return 'left'\n",
    "\n",
    "            return 'other'\n",
    "\n",
    "        for party, count in party_dist.items():\n",
    "            classification = classify_party(party)\n",
    "\n",
    "            if classification == 'left':\n",
    "                left_count += count\n",
    "                left_party_details[party] = count\n",
    "            elif classification == 'right':\n",
    "                right_count += count\n",
    "                right_party_details[party] = count\n",
    "            elif classification == 'center':\n",
    "                center_count += count\n",
    "                center_party_details[party] = count\n",
    "            else:\n",
    "                other_count += count\n",
    "                other_party_details[party] = count\n",
    "\n",
    "        total_categorized = left_count + right_count + center_count + other_count\n",
    "\n",
    "        if total_categorized > 0:\n",
    "            print(f\"   Political Spectrum Distribution:\")\n",
    "            print(f\"     \ud83d\udfe6 Left-wing parties: {left_count:,} ({left_count/total_categorized*100:.1f}%)\")\n",
    "            print(f\"     \ud83d\udfe5 Right-wing parties: {right_count:,} ({right_count/total_categorized*100:.1f}%)\")\n",
    "            print(f\"     \ud83d\udfe8 Center parties: {center_count:,} ({center_count/total_categorized*100:.1f}%)\")\n",
    "            print(f\"     \u26aa Other/Unknown: {other_count:,} ({other_count/total_categorized*100:.1f}%)\")\n",
    "\n",
    "            # Show party details\n",
    "            if left_party_details:\n",
    "                print(f\"\\n   Left-wing parties breakdown:\")\n",
    "                for party, count in sorted(left_party_details.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"     \u2022 {party}: {count:,}\")\n",
    "\n",
    "            if right_party_details:\n",
    "                print(f\"\\n   Right-wing parties breakdown:\")\n",
    "                for party, count in sorted(right_party_details.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"     \u2022 {party}: {count:,}\")\n",
    "\n",
    "            if center_party_details:\n",
    "                print(f\"\\n   Center parties breakdown:\")\n",
    "                for party, count in sorted(center_party_details.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"     \u2022 {party}: {count:,}\")\n",
    "\n",
    "            # Create visualization for left vs right comparison\n",
    "            import plotly.graph_objects as go\n",
    "\n",
    "            fig_spectrum = go.Figure(data=[\n",
    "                go.Bar(\n",
    "                    name='Left-wing',\n",
    "                    x=['Political Spectrum'],\n",
    "                    y=[left_count],\n",
    "                    marker_color='blue'\n",
    "                ),\n",
    "                go.Bar(\n",
    "                    name='Right-wing',\n",
    "                    x=['Political Spectrum'],\n",
    "                    y=[right_count],\n",
    "                    marker_color='red'\n",
    "                ),\n",
    "                go.Bar(\n",
    "                    name='Center',\n",
    "                    x=['Political Spectrum'],\n",
    "                    y=[center_count],\n",
    "                    marker_color='yellow'\n",
    "                ),\n",
    "                go.Bar(\n",
    "                    name='Other',\n",
    "                    x=['Political Spectrum'],\n",
    "                    y=[other_count],\n",
    "                    marker_color='gray'\n",
    "                )\n",
    "            ])\n",
    "\n",
    "            fig_spectrum.update_layout(\n",
    "                title='\u2696\ufe0f Political Spectrum Distribution',\n",
    "                barmode='stack',\n",
    "                height=500,\n",
    "                showlegend=True\n",
    "            )\n",
    "\n",
    "            fig_spectrum.show()\n",
    "\n",
    "            # Save the figure\n",
    "            fig_spectrum.write_html(FIGURES_DIR / 'political_spectrum_analysis.html')\n",
    "\n",
    "            # Calculate ratio\n",
    "            if right_count > 0:\n",
    "                lr_ratio = left_count / right_count\n",
    "                print(f\"\\n   Left/Right Ratio: {lr_ratio:.2f}\")\n",
    "                if lr_ratio > 1:\n",
    "                    print(f\"   \ud83d\udcc8 Left-wing parties have {lr_ratio:.1f}x more representation than right-wing\")\n",
    "                elif lr_ratio < 1:\n",
    "                    print(f\"   \ud83d\udcc9 Right-wing parties have {1/lr_ratio:.1f}x more representation than left-wing\")\n",
    "                else:\n",
    "                    print(f\"   \u2696\ufe0f Balanced representation between left and right-wing parties\")\n",
    "\n",
    "        # Party by municipality visualization\n",
    "        if 'party_by_municipality' in party_analysis:\n",
    "            party_muni_data = party_analysis['party_by_municipality']\n",
    "\n",
    "            # Create heatmap for party-municipality distribution\n",
    "            if party_muni_data:\n",
    "                party_muni_df = pd.DataFrame(party_muni_data).fillna(0)\n",
    "\n",
    "                fig_party_heatmap = px.imshow(\n",
    "                    party_muni_df.values,\n",
    "                    x=party_muni_df.columns,\n",
    "                    y=party_muni_df.index,\n",
    "                    title='\ud83d\uddf3\ufe0f Political Party Distribution by Municipality',\n",
    "                    labels={'x': 'Political Party', 'y': 'Municipality', 'color': 'Count'},\n",
    "                    aspect='auto',\n",
    "                    color_continuous_scale='Reds'\n",
    "                )\n",
    "\n",
    "                fig_party_heatmap.update_layout(height=600)\n",
    "                fig_party_heatmap.show()\n",
    "\n",
    "                fig_party_heatmap.write_html(FIGURES_DIR / 'political_party_by_municipality.html')\n",
    "\n",
    "    # Presence analysis\n",
    "    if 'presence_analysis' in metadata_analysis:\n",
    "        presence_analysis = metadata_analysis['presence_analysis']\n",
    "        print(f\"\\n\ud83d\udc65 Presence Analysis:\")\n",
    "        print(f\"   Total with presence info: {presence_analysis.get('total_with_presence_info', 0):,}\")\n",
    "\n",
    "        presence_types = presence_analysis.get('presence_types', {})\n",
    "        presence_pct = presence_analysis.get('presence_percentages', {})\n",
    "\n",
    "        for presence_type, count in presence_types.items():\n",
    "            pct = presence_pct.get(presence_type, 0)\n",
    "            print(f\"     {presence_type}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # Schedule analysis\n",
    "    if 'schedule_analysis' in metadata_analysis:\n",
    "        schedule_analysis = metadata_analysis['schedule_analysis']\n",
    "        print(f\"\\n\ud83d\udd52 Schedule Analysis:\")\n",
    "        print(f\"   Total with schedule info: {schedule_analysis.get('total_with_schedule', 0):,}\")\n",
    "        print(f\"   Documents with schedule: {schedule_analysis.get('documents_with_schedule', 0):,}\")\n",
    "\n",
    "        schedule_patterns = schedule_analysis.get('schedule_patterns', {})\n",
    "        if schedule_patterns:\n",
    "            print(f\"\\n   Most Common Schedule Patterns:\")\n",
    "            for schedule, count in list(schedule_patterns.items())[:10]:\n",
    "                print(f\"     {schedule}: {count} times\")\n",
    "\n",
    "    print(\"\ud83d\udcbe Metadata analysis completed\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No metadata available for analysis\")\n",
    "    if 'error' in metadata_analysis:\n",
    "        print(f\"   Error: {metadata_analysis['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axt1t5y7tnv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the metadata analysis - remove purple bar and fix year scale\n",
    "print(\"=== METADATA ANALYSIS ===\")\n",
    "\n",
    "# Participation analysis\n",
    "participantes_data = entities_df[entities_df['participantes'].notna()]\n",
    "horario_data = entities_df[entities_df['horario'].notna()]\n",
    "\n",
    "if not participantes_data.empty or not horario_data.empty:\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Create 2-panel layout (remove the meaningless document distribution heatmap)\n",
    "    fig_metadata = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Participation by Municipality', 'Schedule Patterns'),\n",
    "        specs=[[{'type': 'heatmap'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Participation by municipality heatmap\n",
    "    if not participantes_data.empty:\n",
    "        participation_matrix = participantes_data.groupby(['municipality', 'participantes']).size().unstack(fill_value=0)\n",
    "        \n",
    "        fig_metadata.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=participation_matrix.values,\n",
    "                x=list(participation_matrix.columns),\n",
    "                y=list(participation_matrix.index),\n",
    "                colorscale='Blues',\n",
    "                name='Participation',\n",
    "                showscale=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\ud83d\udc65 Participation Analysis:\")\n",
    "        print(f\"   - Total entities with participants: {len(participantes_data):,}\")\n",
    "        print(f\"   - Documents with participants: {participantes_data['filename'].nunique()}\")\n",
    "        print(f\"   - Unique participants: {participantes_data['participantes'].nunique()}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Participation by Municipality:\")\n",
    "        for municipality in participation_matrix.index:\n",
    "            total = participation_matrix.loc[municipality].sum()\n",
    "            print(f\"   - {municipality}: {total} participant mentions\")\n",
    "    \n",
    "    # 2. Schedule patterns bar chart\n",
    "    if not horario_data.empty:\n",
    "        schedule_counts = horario_data['horario'].value_counts().head(10)\n",
    "        \n",
    "        fig_metadata.add_trace(\n",
    "            go.Bar(\n",
    "                x=list(schedule_counts.index),\n",
    "                y=list(schedule_counts.values),\n",
    "                name='Schedule Frequency',\n",
    "                marker_color='orange',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd52 Schedule Analysis:\")\n",
    "        print(f\"   - Total entities with schedule: {len(horario_data):,}\")\n",
    "        print(f\"   - Documents with schedule: {horario_data['filename'].nunique()}\")\n",
    "        print(f\"   - Top schedule patterns:\")\n",
    "        for schedule, count in schedule_counts.head(5).items():\n",
    "            print(f\"     \u2022 {schedule}: {count} occurrences\")\n",
    "    \n",
    "    fig_metadata.update_layout(\n",
    "        height=600, \n",
    "        title_text=\"Metadata Analysis - Participation and Schedule Patterns\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig_metadata.show()\n",
    "    \n",
    "    # Show document distribution as a simple table instead of problematic heatmap\n",
    "    print(f\"\\n\ud83d\udcc5 Document Distribution Summary (Fixed Date Issues):\")\n",
    "    \n",
    "    # Create corrected year data\n",
    "    documents_df_fixed = documents_df.copy()\n",
    "    \n",
    "    def fix_date_format(date_str):\n",
    "        if pd.isna(date_str) or '- C\u00f3pia' in str(date_str):\n",
    "            return None\n",
    "        try:\n",
    "            parts = str(date_str).split('-')\n",
    "            if len(parts) == 3:\n",
    "                year, part1, part2 = parts\n",
    "                if int(part1) > 12:  # day-month format, swap them\n",
    "                    return f\"{year}-{part2}-{part1}\"\n",
    "                else:\n",
    "                    return date_str\n",
    "        except:\n",
    "            return None\n",
    "        return date_str\n",
    "    \n",
    "    documents_df_fixed['date_corrected'] = documents_df_fixed['date'].apply(fix_date_format)\n",
    "    documents_df_fixed['year_corrected'] = pd.to_datetime(documents_df_fixed['date_corrected'], errors='coerce').dt.year\n",
    "    \n",
    "    # Filter out duplicates and show clean distribution\n",
    "    clean_docs = documents_df_fixed[~documents_df_fixed['filename'].str.contains('- C\u00f3pia', na=False)]\n",
    "    \n",
    "    doc_time_dist_clean = clean_docs.groupby(['municipality', 'year_corrected']).size().unstack(fill_value=0)\n",
    "    \n",
    "    print(\"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\")\n",
    "    print(\"\u2502Municipality \u2502 2021 \u2502 2022 \u2502 2023 \u2502 2024 \u2502 Total \u2502\")\n",
    "    print(\"\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\")\n",
    "    \n",
    "    for municipality in doc_time_dist_clean.index:\n",
    "        row_data = doc_time_dist_clean.loc[municipality]\n",
    "        total = row_data.sum()\n",
    "        print(f\"\u2502 {municipality:<11} \u2502  {row_data.get(2021.0, 0):>2}  \u2502  {row_data.get(2022.0, 0):>2}  \u2502  {row_data.get(2023.0, 0):>2}  \u2502  {row_data.get(2024.0, 0):>2}  \u2502  {total:>3}  \u2502\")\n",
    "    \n",
    "    print(\"\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\")\n",
    "    \n",
    "    total_docs = clean_docs.shape[0]\n",
    "    total_with_valid_dates = clean_docs['year_corrected'].notna().sum()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Summary:\")\n",
    "    print(f\"   - Total documents (excluding duplicates): {total_docs}\")\n",
    "    print(f\"   - Documents with valid dates: {total_with_valid_dates}\")\n",
    "    print(f\"   - Documents with date issues: {total_docs - total_with_valid_dates}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No metadata available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "publication-summary",
   "metadata": {},
   "source": [
    "## 12. Publication-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "publication-tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===  SUMMARY ===\")\n",
    "\n",
    "# Dataset overview table\n",
    "corpus_stats = comprehensive_analysis['corpus_statistics']['corpus_overview']\n",
    "entity_stats = comprehensive_analysis['corpus_statistics'].get('entity_overview', {})\n",
    "relation_stats = comprehensive_analysis['corpus_statistics'].get('relation_overview', {})\n",
    "\n",
    "# Table 1: Dataset Characteristics\n",
    "dataset_char_data = {\n",
    "    'Characteristic': [\n",
    "        'Documents',\n",
    "        'Municipalities', \n",
    "        'Total Characters',\n",
    "        'Total Tokens',\n",
    "        'Avg Tokens/Document',\n",
    "        'Total Entities',\n",
    "        'Entity Types',\n",
    "        'Total Relations',\n",
    "        'Entity Coverage (%)',\n",
    "        'Avg Entities/Document'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{corpus_stats['total_documents']:,}\",\n",
    "        f\"{corpus_stats['total_municipalities']}\",\n",
    "        f\"{corpus_stats['total_text_length']:,}\",\n",
    "        f\"{corpus_stats['total_tokens']:,}\",\n",
    "        f\"{corpus_stats['total_tokens'] / corpus_stats['total_documents']:.0f}\",\n",
    "        f\"{entity_stats.get('total_entities', 0):,}\",\n",
    "        f\"{entity_stats.get('unique_entity_types', 0)}\",\n",
    "        f\"{relation_stats.get('total_relations', 0):,}\",\n",
    "        f\"{entity_stats.get('entity_coverage', 0)*100:.1f}\",\n",
    "        f\"{entity_stats.get('avg_entities_per_document', 0):.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset_char_df = pd.DataFrame(dataset_char_data)\n",
    "print(\"\\nTable 1: Dataset Characteristics\")\n",
    "print(dataset_char_df.to_string(index=False))\n",
    "\n",
    "# Save as CSV and LaTeX\n",
    "dataset_char_df.to_csv(STATISTICS_DIR / 'table1_dataset_characteristics.csv', index=False)\n",
    "with open(STATISTICS_DIR / 'table1_dataset_characteristics.tex', 'w') as f:\n",
    "    f.write(dataset_char_df.to_latex(index=False, caption=\"Dataset Characteristics\", label=\"tab:dataset_char\"))\n",
    "\n",
    "# Table 2: Entity Type Distribution\n",
    "if not entities_df.empty:\n",
    "    entity_type_stats = entities_df['entity_label'].value_counts()\n",
    "    entity_type_pct = (entity_type_stats / entity_type_stats.sum() * 100).round(2)\n",
    "    \n",
    "    entity_dist_data = {\n",
    "        'Entity Type': entity_type_stats.index.tolist(),\n",
    "        'Count': entity_type_stats.values.tolist(),\n",
    "        'Percentage': [f\"{pct:.1f}%\" for pct in entity_type_pct.values]\n",
    "    }\n",
    "    \n",
    "    entity_dist_df = pd.DataFrame(entity_dist_data)\n",
    "    print(\"\\nTable 2: Entity Type Distribution\")\n",
    "    print(entity_dist_df.to_string(index=False))\n",
    "    \n",
    "    entity_dist_df.to_csv(STATISTICS_DIR / 'table2_entity_distribution.csv', index=False)\n",
    "    with open(STATISTICS_DIR / 'table2_entity_distribution.tex', 'w') as f:\n",
    "        f.write(entity_dist_df.to_latex(index=False, caption=\"Entity Type Distribution\", label=\"tab:entity_dist\"))\n",
    "\n",
    "# Table 3: Municipality Statistics  \n",
    "muni_stats_data = []\n",
    "for municipality in documents_df['municipality'].unique():\n",
    "    muni_docs = documents_df[documents_df['municipality'] == municipality]\n",
    "    muni_entities = entities_df[entities_df['municipality'] == municipality] if not entities_df.empty else pd.DataFrame()\n",
    "    \n",
    "    muni_stats_data.append({\n",
    "        'Municipality': municipality,\n",
    "        'Documents': len(muni_docs),\n",
    "        'Total Tokens': muni_docs['token_count'].sum(),\n",
    "        'Total Entities': len(muni_entities),\n",
    "        'Entities/Document': len(muni_entities) / len(muni_docs) if len(muni_docs) > 0 else 0,\n",
    "        'Entity Density': len(muni_entities) / muni_docs['token_count'].sum() if muni_docs['token_count'].sum() > 0 else 0\n",
    "    })\n",
    "\n",
    "muni_stats_df = pd.DataFrame(muni_stats_data)\n",
    "muni_stats_df['Entities/Document'] = muni_stats_df['Entities/Document'].round(2)\n",
    "muni_stats_df['Entity Density'] = muni_stats_df['Entity Density'].round(4)\n",
    "\n",
    "print(\"\\nTable 3: Municipality Statistics\")\n",
    "print(muni_stats_df.to_string(index=False))\n",
    "\n",
    "muni_stats_df.to_csv(STATISTICS_DIR / 'table3_municipality_statistics.csv', index=False)\n",
    "with open(STATISTICS_DIR / 'table3_municipality_statistics.tex', 'w') as f:\n",
    "    f.write(muni_stats_df.to_latex(index=False, caption=\"Municipality Statistics\", label=\"tab:muni_stats\"))\n",
    "\n",
    "print(\"\\n\ud83d\udcbe All tables saved to LaTeX and CSV formats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}